{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up0AORHdv_Gh"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn71OSaxLu3p"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install memory_profiler\n",
        "!pip install keras-multi-head\n",
        "!pip install ipython-autotime\n",
        "!pip install keras_nlp\n",
        "!pip install keras-utilities\n",
        "\n",
        "# Load Time and memory measurements\n",
        "%load_ext autotime\n",
        "%load_ext memory_profiler\n",
        "\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "import numpy as np, json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, sys, argparse, time, h5py\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "from kutilities.layers import MeanOverTime\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score, f1_score\n",
        "import tensorflow as tf  \n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "global seed\n",
        "seed = 1337\n",
        "np.random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If run on Google colab and the datasets are in your Google drive, connect your Google drive with this notebook\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unpack the required datasets, change the folder if necessary.\n",
        "!unzip drive/MyDrive/ZIPYOUTUBE.zip -d /content/\n",
        "!unzip drive/MyDrive/ZIPMMMO.zip -d /content/\n",
        "!unzip drive/MyDrive/ZIPMOUD.zip -d /content/\n",
        "!unzip drive/MyDrive/ZIPMOSI.zip -d /content/"
      ],
      "metadata": {
        "id": "bf9JuYG9jOC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(prediction, test_label, print_detailed_results=False):\n",
        "    \"\"\"\n",
        "    This function calculates the Accuracy and F1-score of the prediction using this as input:\n",
        "    - prediction - which containts the predicted probabilities \n",
        "    - test_label - which containts the true labels\n",
        "    print_detailed_results - a boolean, which indicates if the confusion matric and Classification report should be reported\n",
        "    \"\"\"\n",
        "\n",
        "    true_label=[]\n",
        "    predicted_label=[]\n",
        "\n",
        "    for i in range(prediction.shape[0]):\n",
        "        true_label.append(np.argmax(test_label[i] ))\n",
        "        predicted_label.append(np.argmax(prediction[i] ))\n",
        "    f_score = round(f1_score(np.round(prediction),np.round(test_label),average='weighted'),5)\n",
        "\n",
        "    if print_detailed_results:\n",
        "        print (\"Confusion Matrix :\")\n",
        "        print (confusion_matrix(true_label, predicted_label))\n",
        "        print (\"Classification Report :\")\n",
        "        print (classification_report(true_label, predicted_label))\n",
        "\n",
        "    return accuracy_score(true_label, predicted_label), f_score\n",
        "\n",
        "def CAM(x, y):\n",
        "    \"\"\"\n",
        "    This is the  Context-aware Attention Module.\n",
        "    This function takes in two inputs, 'x' and 'y', two modalities. \n",
        "    A dot product of the inputs along the last two axes is applied, resulting into m_dash. \n",
        "    A softmax activation function is applied to m_dash, resulting in m. Then, a dot product of m and y is taken along the last two axes.\n",
        "    The resulting h_dash is then multiplied element-wise with x.\n",
        "    \"\"\"\n",
        "    m_dash = dot([x, y], axes=[2,2])\n",
        "    m = Activation('softmax')(m_dash)\n",
        "    h_dash = dot([m, y], axes=[2,1])\n",
        "    return multiply([h_dash, x])"
      ],
      "metadata": {
        "id": "1SYJdNHIUEV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-R0Oo8nB7l6"
      },
      "outputs": [],
      "source": [
        "def featuresExtraction(dataset, classNo):\n",
        "    \"\"\"\n",
        "        This function extracts the features using as input the dataset name and the number of classes. \n",
        "        There are 4 Datasets with the corresponding classes in brackets: MOSI(2 or 7), MMMO (2), Youtube (3) and MOUD (2). \n",
        "        Audio, video and text features are extracted for training, validation and test set. \n",
        "    \"\"\"\n",
        "    global train_text, train_audio, train_video, train_label\n",
        "    global valid_text, valid_audio, valid_video, valid_label\n",
        "    global test_text, test_audio, test_video, test_label\n",
        "\n",
        "    if dataset == 'MOSI': \n",
        "        hf = h5py.File('/content/MOSI/X_train.h5','r')\n",
        "        X_train = hf['data'][:]\n",
        "        split_into_features_train = np.split(X_train, [300, 305, 325], axis=2)\n",
        "        train_text = split_into_features_train[0]\n",
        "        train_audio = split_into_features_train[1]\n",
        "        train_video = split_into_features_train[2]\n",
        "\n",
        "        hf = h5py.File('/content/MOSI/X_test.h5','r')\n",
        "        X_test = hf['data'][:]\n",
        "        split_into_features_test = np.split(X_test, [300, 305, 325], axis=2)\n",
        "        test_text = split_into_features_test[0]\n",
        "        test_audio = split_into_features_test[1]\n",
        "        test_video = split_into_features_test[2]\n",
        "\n",
        "        hf = h5py.File('/content/MOSI/X_valid.h5','r')\n",
        "        X_valid = hf['data'][:]\n",
        "        split_into_features_valid = np.split(X_valid, [300, 305, 325], axis=2)\n",
        "        valid_text = split_into_features_valid[0]\n",
        "        valid_audio = split_into_features_valid[1]\n",
        "        valid_video = split_into_features_valid[2]\n",
        "\n",
        "        hf = h5py.File('/content/MOSI/y_train.h5','r')\n",
        "        y_train = hf['data'][:]\n",
        "\n",
        "        hf = h5py.File('/content/MOSI/y_test.h5','r')\n",
        "        y_test = hf['data'][:]\n",
        "\n",
        "        hf = h5py.File('/content/MOSI/y_valid.h5','r')\n",
        "        y_valid = hf['data'][:]\n",
        "\n",
        "        if classNo == 2:\n",
        "\n",
        "            test_label = [[1, 0] if val < 0 else [0, 1] for val in y_test]\n",
        "            test_label = np.array(test_label)\n",
        "\n",
        "            train_label = [[1, 0] if val < 0 else [0, 1] for val in y_train]\n",
        "            train_label = np.array(train_label)\n",
        "\n",
        "            valid_label = [[1, 0] if val < 0 else [0, 1] for val in y_valid]\n",
        "            valid_label = np.array(valid_label)\n",
        "\n",
        "        if classNo == 7: \n",
        "\n",
        "            y_train = np.round(y_train)\n",
        "            train_label = to_categorical(y_train - y_train.min())\n",
        "\n",
        "            y_test = np.round(y_test)\n",
        "            test_label = to_categorical(y_test - y_test.min())\n",
        "\n",
        "            y_valid = np.round(y_valid)\n",
        "            valid_label = to_categorical(y_valid - y_valid.min())\n",
        "\n",
        "    else:\n",
        "\n",
        "          with open('/content/' + dataset + '/covarep_train.p', 'rb') as f:\n",
        "              train_audio = pickle.load(f, encoding='latin1')\n",
        "          train_audio = train_audio/np.max(train_audio)\n",
        "\n",
        "          with open('/content/' + dataset + '/covarep_valid.p', 'rb') as f:\n",
        "              valid_audio = pickle.load(f, encoding='latin1')\n",
        "          valid_audio = valid_audio/np.max(valid_audio)\n",
        "\n",
        "\n",
        "          with open('/content/' + dataset + '/covarep_test.p', 'rb') as f:\n",
        "              test_audio = pickle.load(f, encoding='latin1')\n",
        "          test_audio  = test_audio/np.max(test_audio)\n",
        "\n",
        "          with open ('/content/' + dataset + '/facet_train.p', 'rb') as f:\n",
        "              train_video = pickle.load(f, encoding='latin1')\n",
        "          train_video = train_video/np.max(train_video)\n",
        "\n",
        "          with open ('/content/' + dataset + '/facet_valid.p', 'rb') as f:\n",
        "              valid_video = pickle.load(f, encoding='latin1')\n",
        "          valid_video = valid_video/np.max(valid_video)\n",
        "\n",
        "          with open('/content/' + dataset + '/facet_test.p', 'rb') as f:\n",
        "              test_video  = pickle.load(f, encoding='latin1')\n",
        "          test_video  = test_video/np.max(test_video)\n",
        "\n",
        "          with open('/content/' + dataset + '/text_train.p', 'rb') as f:\n",
        "              train_text  = pickle.load(f, encoding='latin1')\n",
        "          train_text  = train_text/np.max(train_text)\n",
        "\n",
        "          with open('/content/' + dataset + '/text_valid.p', 'rb') as f:\n",
        "              valid_text  = pickle.load(f, encoding='latin1')\n",
        "          valid_text  = valid_text/np.max(valid_text)\n",
        "\n",
        "          with open('/content/' + dataset + '/text_test.p', 'rb') as f:\n",
        "              test_text   = pickle.load(f, encoding='latin1')\n",
        "          test_text   = test_text/np.max(test_text)\n",
        "\n",
        "          if dataset == 'MMMO':\n",
        "                with open('/content/' + dataset + '/y_train.p', 'rb') as f:\n",
        "                    train_label = pickle.load(f, encoding='latin1')\n",
        "                train_label = [[1, 0] if val <= 3.5 else [0, 1] for val in train_label]\n",
        "                train_label = np.array(train_label)\n",
        "\n",
        "                with open('/content/' + dataset + '/y_valid.p', 'rb') as f:\n",
        "                    valid_label = pickle.load(f, encoding='latin1')\n",
        "                valid_label = [[1, 0] if val <= 3.5 else [0, 1] for val in valid_label]\n",
        "                valid_label = np.array(valid_label)\n",
        "\n",
        "                with open('/content/' + dataset + '/y_test.p', 'rb') as f:\n",
        "                    test_label  = pickle.load(f, encoding='latin1')\n",
        "                test_label = [[1, 0] if val <= 3.5 else [0, 1] for val in test_label]\n",
        "                test_label = np.array(test_label)\n",
        "          else:\n",
        "          \n",
        "              with open('/content/' + dataset + '/y_train.p', 'rb') as f:\n",
        "                  train_label = pickle.load(f, encoding='latin1')\n",
        "              with open('/content/' + dataset + '/y_valid.p', 'rb') as f:\n",
        "                  valid_label = pickle.load(f, encoding='latin1')\n",
        "              with open('/content/' + dataset + '/y_test.p', 'rb') as f:\n",
        "                  test_label  = pickle.load(f, encoding='latin1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f_H11yMLeDt"
      },
      "source": [
        "# Proposed Multi-head self-attention with context-aware attention model (MHCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SReI-e3pSbYs"
      },
      "outputs": [],
      "source": [
        "global seed\n",
        "seed = 1337\n",
        "np.random.seed(seed)\n",
        "def MHCA_model(dataset,classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the Proposed Multi-head self-attention with context-aware attention model (MHCA). \n",
        "        The MHCA model consists of three main components: a Multi-Head Attention (MHA) module, a fully connected layer \n",
        "        with dropout (Dense Layer + Dropout), and a Context-aware Attention (CAM) module.\n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys           = 'text_audio_video' \n",
        "    runs                = 1\n",
        "    acc       = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\n",
        "        in_test_label   = []\t\t\n",
        "\t\t    # Shape of Modalitys\n",
        "        text_shape      = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape     = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape     = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "\n",
        "        # ============ Multi-Head Attention (MHA) Module  ======================\n",
        "        text_MHA        = MultiHeadAttention(head_num =5)(text_shape)           # All (5)\n",
        "        audio_MHA       = MultiHeadAttention(head_num =2)(audio_shape)          # MOSI (5), YOUTUBE, MOUD, MMMO (2)\n",
        "        video_MHA       = MultiHeadAttention(head_num =7)(video_shape)          # MOSI (5), YOUTUBE, MOUD, MMMO (7)\n",
        "\n",
        "        # ============ Dense Module ============================================\n",
        "        text_MHA_fc     = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh' ))(text_MHA))\n",
        "        audio_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_MHA))\n",
        "        video_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_MHA))\n",
        "\n",
        "        # ============  Context-aware Attention (CAM) Module ===================\n",
        "        cam_TA          = CAM(text_MHA_fc, audio_MHA_fc)\n",
        "        cam_TV          = CAM(text_MHA_fc, video_MHA_fc)\n",
        "        cam_AV          = CAM(audio_MHA_fc, video_MHA_fc)\n",
        "\n",
        "\t\t    # ============ Concatenation ===========================================\t\n",
        "\n",
        "        merged          = concatenate([cam_TA,cam_TV,cam_AV])\n",
        "        merged          = Dense(neurons, activation='relu')(merged)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model           = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                                outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "        #model.summary()\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + time +'_' +dataset + '_' + str(modalitys)+'_' + str(drop) + '_' + str(neurons)+ '_' +str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train_text,train_audio,train_video], \n",
        "                                  [train_audio,train_video,train_text,train_label],\n",
        "                                   epochs=50,\n",
        "                                   batch_size=2,\n",
        "                                   shuffle=True,\n",
        "                                   callbacks=[check1, check2],\n",
        "                                   validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                                   [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                                    verbose=0)\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test_text,test_audio,test_video])\n",
        "        #model.summary()\n",
        "\n",
        "        result[-1]      = np.nan_to_num(result[-1])\n",
        "        test_label_     = [array.tolist() for array in test_label]\n",
        "        acc, f1_score   = calculate_accuracy(result[-1], test_label_, True)\n",
        "\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "\n",
        "        open('/content/drive/MyDrive/results/'+  dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) + '.txt', 'a').write(str(acc)+'_' + str(time)+'_' + str(run)  + '\\n')  \n",
        "        with open('/content/drive/MyDrive/history/'+  time + dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) +'_'+str(run)+'_'+'history.pkl', 'wb') as file:\n",
        "            pickle.dump(history.history, file)\n",
        "        print(path)\n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "  \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()   \n",
        "        %load_ext autotime\n",
        "\n",
        "       # ============ Set time (For distinction later), dataset, number of classes, the droprate and number of neurons in the Dense layer with Dropout =============================\n",
        "\n",
        "time = '13:26'\n",
        "dataset                 = 'YOUTUBE'                                             # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'MHA_Dense_CAM'       \n",
        "classNo                 = 3                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.3                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 50                                                    # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)\n",
        "%memit (MHCA_model(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "LJfy1y0pfC7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unimodal vs. Bimodal vs. Trimodal"
      ],
      "metadata": {
        "id": "-0kmGIwpGvFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uni-modality"
      ],
      "metadata": {
        "id": "rC9ziHzoV8Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MHCA_Unimodal(dataset,classNo, drop, neurons, modality):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model with using only one modality (uni-modal) using\n",
        "        the inputs: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), the number of neurons in the dense layer (neurons), and the selected modality for training uni-modal.\n",
        "    \"\"\"\n",
        "    if modality == 'text':\n",
        "        train = train_text\n",
        "        valid = valid_text\n",
        "        test = test_text\n",
        "    if modality == 'audio':\n",
        "        train = train_audio\n",
        "        valid = valid_audio\n",
        "        test = test_audio\n",
        "    if modality == 'video':\n",
        "        train = train_video\n",
        "        valid = valid_video\n",
        "        test = test_video\n",
        "\n",
        "    if dataset == 'MOSI':\n",
        "        heads = 5\n",
        "    elif modality == 'text':\n",
        "        heads = 5\n",
        "    elif modality == 'audio':\n",
        "        heads = 2\n",
        "    elif modality == 'video':\n",
        "        heads = 7        \n",
        "\n",
        "    runs                = 1\n",
        "    acc                 = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "        in_test_label   = []\t\t\n",
        "\t\t    # Shape of Modalitys\n",
        "        shape           = Input(shape=(train.shape[1], train.shape[2]))\n",
        "\n",
        "        # =================== Multi-Head Attention Module ======================\n",
        "        MHA        = MultiHeadAttention(head_num = heads)(shape)\n",
        "        # ============ Dense Module ============================================\n",
        "        MHA_fc     = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(MHA))\n",
        "\n",
        "        # ===============  Context-aware Attention (CAM) Module ================\n",
        "        cam_uniModal          = CAM(MHA, MHA)\n",
        "\t    \t# ==================== Concatenation ===================================\t\n",
        "        merged          = Dense(neurons, activation='relu')(cam_uniModal)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ======================== Model =======================================\n",
        "        model           = Model(inputs=shape,\n",
        "                                outputs=final_output)\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + time +'_' +dataset + '_' + str(modality)+'_' + str(drop) + '_' + str(neurons)+ '_' +str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit(train, train_label,\n",
        "                                    epochs=50,\n",
        "                                    batch_size=2,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=(valid, valid_label),\n",
        "                                    verbose=0)\n",
        "        #model.load_weights(path)\n",
        "        result          = model.predict(test)\n",
        "        result = np.nan_to_num(result)\n",
        "        test_label_     = [array.tolist() for array in test_label]\n",
        "\n",
        "\n",
        "        acc, f1_score   = calculate_accuracy(result, test_label, True)\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "        open('/content/drive/MyDrive/results/'+  dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) + '.txt', 'a').write(str(acc)+'_' + str(time)+'_' + str(run) + '_' + '\\n')  \n",
        "        with open('/content/drive/MyDrive/history/'+  time +dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) +'_'+str(run)+'_'+'history.pkl', 'wb') as file:\n",
        "            pickle.dump(history.history, file)\n",
        "        print(path)\n",
        "\n",
        "  \n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['acc'])\n",
        "        ax1.plot(history.history['val_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "        plt.show()   \n",
        "        %load_ext autotime\n",
        "\n",
        "# Set time (For distinction later), dataset, number of classes, the droprate and number of neurons in the Dense layer with Dropout \n",
        "\n",
        "time                    = '15:23'\n",
        "dataset                 = 'MOSI'  \n",
        "classNo                 = 2\n",
        "modality                = 'audio'\n",
        "modelType               = 'MHCA_Unimodal' + '_' + modality\n",
        "drop                    = 0.3\n",
        "neurons                 = 50\n",
        "\n",
        "featuresExtraction(dataset, classNo)  \n",
        "%load_ext memory_profiler\n",
        "%memit (MHCA_Unimodal(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons, modality=modality))"
      ],
      "metadata": {
        "id": "itS5jzhMGtsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bi-modality"
      ],
      "metadata": {
        "id": "RGOZ_xd_V5IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_and_heads(modality, dataset):\n",
        "    \"\"\"\n",
        "        This function is to get the data and number of heads corresponding to the given modality using the inputs: the selected dataset (dataset),\n",
        "        and the selected modality\n",
        "    \"\"\"\n",
        "    if modality == 'text':\n",
        "        train = train_text\n",
        "        valid = valid_text\n",
        "        test = test_text\n",
        "    elif modality == 'audio':\n",
        "        train = train_audio\n",
        "        valid = valid_audio\n",
        "        test = test_audio\n",
        "    elif modality == 'video':\n",
        "        train = train_video\n",
        "        valid = valid_video\n",
        "        test = test_video\n",
        "\n",
        "    if dataset == 'MOSI':\n",
        "        heads = 5\n",
        "    elif modality == 'text':\n",
        "        heads = 5\n",
        "    elif modality == 'audio':\n",
        "        heads = 2\n",
        "    elif modality == 'video':\n",
        "        heads = 7\n",
        "\n",
        "    return train, valid, test, heads\n",
        "\n",
        "\n",
        "\n",
        "def MHCA_Bimodal(dataset,classNo, drop, neurons, modality1, modality2):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model with using two modalities (bi-modal) using\n",
        "        the inputs: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), the number of neurons in the dense layer (neurons), and the selected modalitys for training bi-modal.\n",
        "    \"\"\"\n",
        "    train1, valid1, test1, heads1 = get_data_and_heads(modality1, dataset) \n",
        "    train2, valid2, test2, heads2 = get_data_and_heads(modality2, dataset)\n",
        "    runs                = 1\n",
        "    acc                 = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "        in_test_label   = []\t\t\n",
        "\t\t    # Shape of Modalitys\n",
        "        shape1          = Input(shape=(train1.shape[1], train1.shape[2]))\n",
        "        shape2          = Input(shape=(train2.shape[1], train2.shape[2]))\n",
        "\n",
        "        # ============ Multi-Head Attention Module ======================\n",
        "        MHA1            = MultiHeadAttention(head_num =heads1)(shape1)\n",
        "        MHA2            = MultiHeadAttention(head_num =heads2)(shape2)\n",
        "        # ============ Dense Module ============================================\n",
        "\n",
        "        MHA_fc_1        = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(MHA1))\n",
        "        MHA_fc_2        = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(MHA2))\n",
        "\n",
        "\t\t    # ============  Context-aware Attention Module ======================\n",
        "        cam             = CAM(MHA_fc_1, MHA_fc_2)\n",
        "\n",
        "\t\t    # ============  Concatenate ======================\t\n",
        "        merged          = Dense(neurons, activation='relu')(cam)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ======================================= Model ===============================================\n",
        "\n",
        "        model           = Model(inputs=[shape1,shape2],\n",
        "                                outputs=[shape2,shape1,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + time +'_' +dataset + '_' + str(modality1)+'_'+str(modality2)+'_' + str(drop) + '_' + str(neurons)+ '_' +str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train1,train2], \n",
        "                                    [train2,train1,train_label],\n",
        "                                    epochs=50,\n",
        "                                    batch_size=2,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=([valid1,valid2], \n",
        "                                    [valid2,valid1,valid_label]),\n",
        "                                    verbose=0)\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test1,test2])\n",
        "        result[-1]      = np.nan_to_num( result[-1])\n",
        "\n",
        "        acc, f1_score   = calculate_accuracy(result[-1], test_label, True)\n",
        "        print('best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "        open('/content/drive/MyDrive/results/'+  dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) + '.txt', 'a').write(str(acc)+'_' + str(time)+'_' + str(run)  +'_F1_' + str(f1_score) + '\\n')  \n",
        "        with open('/content/drive/MyDrive/history/'+  time +dataset + str(classNo)+ '_' + modelType + '_' + str(drop)+'_'+str(neurons) +'_'+str(run)+'_'+'history.pkl', 'wb') as file:\n",
        "            pickle.dump(history.history, file)\n",
        "        print(path)\n",
        "\n",
        "  \n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()  \n",
        "        %load_ext autotime\n",
        "\t\t\n",
        "time                    = '00:32'\n",
        "dataset                 = 'MOUD'                                                # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modality1               = 'text'\n",
        "modality2               = 'audio'\n",
        "modelType               = 'MHCA_Bimodal' + '_' + modality1 + '_' + modality2\n",
        "classNo                 = 2                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.9                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 100                                                   # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)   # hier ersetzen\n",
        "%load_ext memory_profiler\n",
        "%memit (MHCA_Bimodal(dataset=dataset, classNo=classNo, neurons=neurons, drop=drop, modality1=modality1, modality2=modality2))"
      ],
      "metadata": {
        "id": "yKtsep16PT5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-attention vs. self-attention"
      ],
      "metadata": {
        "id": "zps_BIFlHAUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_multi_head import MultiHeadAttention\n",
        "import time\n",
        "import pickle \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def MHCA_crossModel(dataset,classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model but instead of self-attention it uses cross-attention in the Multi-Head Attention (MHA) module\n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys  = 'text_audio_video' \n",
        "    runs = 1\n",
        "    acc = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\t\t    # Shape of Modalitys\n",
        "        text_shape         = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape        = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape        = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "\n",
        "        # ============ Multi-Head Attention (MHA) Module  ======================\n",
        "        text_audio_MHA     = MultiHeadAttention(head_num =5)([text_shape, audio_shape, audio_shape])\n",
        "        text_video_MHA     = MultiHeadAttention(head_num =5)([text_shape, video_shape, video_shape])\n",
        "        audio_video_MHA    = MultiHeadAttention(head_num =5)([audio_shape, video_shape, video_shape])\n",
        "\n",
        "        audio_text_MHA     = MultiHeadAttention(head_num =5)([audio_shape, text_shape, text_shape])\n",
        "        video_text_MHA     = MultiHeadAttention(head_num =5)([video_shape, text_shape, text_shape])\n",
        "        video_audio_MHA    = MultiHeadAttention(head_num =5)([video_shape, audio_shape, audio_shape])\n",
        "        # ============ Dense Module ============================================\n",
        "\n",
        "        text_audio_MHA_fc  = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(text_audio_MHA))\n",
        "        text_video_MHA_fc  = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(text_video_MHA))\n",
        "        audio_video_MHA_fc = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_video_MHA))\n",
        "\n",
        "\n",
        "        audio_text_MHA_fc  = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_text_MHA))\n",
        "        video_text_MHA_fc  = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_text_MHA))\n",
        "        video_audio_MHA_fc = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_audio_MHA))\n",
        "\n",
        "        # ============  Context-aware Attention (CAM) Module ===================\n",
        "        cam_1              = CAM(text_audio_MHA_fc, text_video_MHA_fc)\n",
        "        cam_2              = CAM(text_audio_MHA_fc, audio_video_MHA_fc)\n",
        "        cam_3              = CAM(text_audio_MHA_fc, audio_text_MHA_fc)\n",
        "        cam_4              = CAM(text_audio_MHA_fc, video_text_MHA_fc)\n",
        "        cam_5              = CAM(text_audio_MHA_fc, video_audio_MHA_fc)\n",
        "\n",
        "        cam_6              = CAM(text_video_MHA_fc,audio_video_MHA_fc )\n",
        "        cam_7              = CAM(text_video_MHA_fc, audio_text_MHA_fc)\n",
        "        cam_8              = CAM(text_video_MHA_fc, video_text_MHA_fc)\n",
        "        cam_9              = CAM(text_video_MHA_fc, video_audio_MHA_fc)\n",
        "\n",
        "        cam_10              = CAM(audio_video_MHA_fc,audio_text_MHA_fc )\n",
        "        cam_11              = CAM(audio_video_MHA_fc, video_text_MHA_fc)\n",
        "        cam_12              = CAM(audio_video_MHA_fc, video_audio_MHA_fc)\n",
        "\n",
        "        cam_13              = CAM(audio_text_MHA_fc, video_text_MHA_fc)\n",
        "        cam_14              = CAM(audio_text_MHA_fc, video_audio_MHA_fc)\n",
        "\n",
        "        cam_15              = CAM(video_text_MHA_fc, video_audio_MHA_fc)\n",
        "    \t\t# ============ Concatenation ===========================================\t\n",
        "        merged              = concatenate([cam_1,cam_2,cam_3,cam_4,cam_5,cam_6,cam_7,cam_8,cam_9,cam_10,cam_11,cam_12, cam_13,cam_14,cam_15])\n",
        "        merged              = Dense(neurons, activation='relu')(merged)\n",
        "        merged              = MeanOverTime()(merged)\n",
        "        final_output        = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model               = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                              outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path                = '/content/drive/MyDrive/weights/' + dataset + str(modalitys)+'_'+str(run)+'.hdf5'\n",
        "        check1              = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2              = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history             = model.fit([train_text,train_audio,train_video], \n",
        "                                        [train_audio,train_video,train_text,train_label],\n",
        "                                        epochs=50,\n",
        "                                        batch_size=16,\n",
        "                                        shuffle=True,\n",
        "                                        callbacks=[check1, check2],\n",
        "                                        validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                        [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                        verbose=1)\n",
        "\n",
        "        model.load_weights(path)\n",
        "        result              = model.predict([test_text,test_audio,test_video])\n",
        "        result[-1]      = np.nan_to_num(result[-1])\n",
        "\n",
        "        #model.summary()\n",
        "        acc, f1_score       = calculate_accuracy(result[-1], test_label, True)\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "        open('/content/drive/MyDrive/results/'+ dataset +'_'+ str(classNo)+ '_' + modelType + '_'+'.txt', 'a').write(str(acc) + '\\n'*2)  \n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "        fig, (ax1, ax2)     = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['acc'])\n",
        "        ax1.plot(history.history['val_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "        plt.show()   \n",
        "        %load_ext autotime\n",
        "\n",
        "dataset                     = 'MOSI'                                            # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType                   = 'Cross_MHA_Dense_CAM'       \n",
        "classNo                     = 2                                                 # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                        = 0.8                                               # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                     = 200                                               # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)   # hier ersetzen\n",
        "MHCA_crossModel(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons)\n",
        "\n"
      ],
      "metadata": {
        "id": "1yUe2w93HBDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYcFZYLeUaY7"
      },
      "source": [
        "### MHA module vs. Transformer\n",
        "\n",
        " The MHA module was replaced with a Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq6DyjFgUeLg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def MHCA_transformer(dataset, classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model but instead of the Multi-Head Attention (MHA) module it uses Transformer encoder\n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys  = 'text_audio_video' \n",
        "    runs = 1\n",
        "    acc = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\t\t # Shape of Modalitys\n",
        "        text_shape                  = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape                  = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape                  = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "        # ============ TransformerEncoder ======================\n",
        "        text_MHA_transformerEncoder  = keras_nlp.layers.TransformerEncoder(intermediate_dim=neurons, num_heads=5, name='TransformerText')\n",
        "        output_text_transformer      = text_MHA_transformerEncoder(text_shape)\n",
        "        \n",
        "        audio_MHA_transformerEncoder = keras_nlp.layers.TransformerEncoder(intermediate_dim=neurons, num_heads=5, name='TransformerAudio')\n",
        "        output_audio_transformer     = audio_MHA_transformerEncoder(audio_shape)\n",
        "\n",
        "        video_MHA_transformerEncoder = keras_nlp.layers.TransformerEncoder(intermediate_dim=neurons, num_heads=5, name='TransformerVisual')\n",
        "        output_video_transformer     = video_MHA_transformerEncoder(video_shape)\n",
        "\n",
        "        # ============ Dense Module ============================================\n",
        "        text_MHA_fc     = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(output_text_transformer))\n",
        "        audio_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(output_audio_transformer))\n",
        "        video_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(output_video_transformer))\n",
        "\n",
        "\n",
        "        # ============  Context-aware Attention (CAM) Module ===================\n",
        "\n",
        "        cam_TA          = CAM(text_MHA_fc, audio_MHA_fc)\n",
        "        cam_TV          = CAM(text_MHA_fc, video_MHA_fc)\n",
        "        cam_AV          = CAM(audio_MHA_fc, video_MHA_fc)\n",
        "\n",
        "        \n",
        "        cam_AT          = CAM(audio_MHA_fc, text_MHA_fc )\n",
        "        cam_VT          = CAM(video_MHA_fc, text_MHA_fc )\n",
        "        cam_VA          = CAM(video_MHA_fc, audio_MHA_fc)\n",
        "\n",
        "\n",
        "        # cam_TA = CAM(output_text_transformer, output_audio_transformer)\n",
        "        # cam_TV = CAM(output_text_transformer, output_video_transformer)\n",
        "        # cam_AV = CAM(output_audio_transformer, output_video_transformer)\n",
        "\n",
        "        \n",
        "        # cam_AT = CAM(output_audio_transformer, output_text_transformer )\n",
        "        # cam_VT = CAM(output_video_transformer, output_text_transformer )\n",
        "        # cam_VA = CAM(output_video_transformer, output_audio_transformer)\n",
        "\n",
        "\n",
        "\t\t# ============ Concatenation ===========================================\t\n",
        "        merged          = concatenate([cam_TA,cam_TV,cam_AV, cam_AT, cam_VT, cam_VA])\n",
        "        merged          = Dense(neurons, activation='relu')(merged)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model           = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                                outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        " \n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_transformer_dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + dataset + str(modalitys)+'_'+str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train_text,train_audio,train_video], \n",
        "                                    [train_audio,train_video,train_text,train_label],\n",
        "                                    epochs=50,\n",
        "                                    batch_size=16,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                    [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                    verbose=1)\n",
        "\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test_text,test_audio,test_video])\n",
        "        result[-1]      = np.nan_to_num(result[-1])\n",
        "\n",
        "        #model.summary()\n",
        "        acc, f1_score   = calculate_accuracy(result[-1], test_label, True)\n",
        "\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "\n",
        "        open('/content/drive/MyDrive/results/'+ dataset + '_' + modelType + '_'+ modalitys+'.txt', 'a').write(modalitys + ', accuracy: ' + str(acc) + '\\n'*2)  \n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "  \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()   \n",
        "        %load_ext autotime\n",
        "\n",
        "  \n",
        "dataset                 = 'MOSI'                                                # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'MHA_Transformer_Dense_CAM'        \n",
        "classNo                 = 7                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.8                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 200                                                   # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)\n",
        "MHCA_transformer(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6n-8U0cFvz-"
      },
      "source": [
        "### Ablation Study: Without MHA Module, Solo CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQuIR3-U1Hfi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def MHCA_withoutMHA(dataset,classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model but without the MHA module. \n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys  = 'text_audio_video' \n",
        "    runs = 1\n",
        "    acc = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\t    # Shape of Modalitys\n",
        "        text_shape      = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape     = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape     = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\t\t\n",
        "        # ============ Dense Module ============================================\n",
        "        text_MHA_fc     = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(text_shape))\n",
        "        audio_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_shape))\n",
        "        video_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_shape))\n",
        "\n",
        "\n",
        "        # ============  Context-aware Attention (CAM) Module ===================\n",
        "        cam_TA          = CAM(text_MHA_fc, audio_MHA_fc)\n",
        "        cam_TV          = CAM(text_MHA_fc, video_MHA_fc)\n",
        "        cam_AV          = CAM(audio_MHA_fc, video_MHA_fc)\n",
        "\n",
        "\n",
        "\t\t# ============ Concatenation ===========================================\t\n",
        "        merged          = concatenate([cam_TA,cam_TV,cam_AV])\n",
        "        merged          = Dense(neurons, activation='relu')(merged)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model           = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                                outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + dataset + str(modalitys)+'_'+str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train_text,train_audio,train_video], \n",
        "                                    [train_audio,train_video,train_text,train_label],\n",
        "                                    epochs=50,\n",
        "                                    batch_size=16,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                    [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                    verbose=1)\n",
        "\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test_text,test_audio,test_video])\n",
        "        #model.summary()\n",
        "        acc, f1_score   = calculate_accuracy(result[-1], test_label, True)\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "\n",
        "        open('/content/drive/MyDrive/results/'+ dataset + '_' + modelType + '_'+ str(drop) + '_' + str(neurons) +'.txt', 'a').write(str(acc) + '\\n'*2)  \n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "  \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()    \n",
        "        %load_ext autotime\n",
        "\n",
        "\n",
        "dataset                 = 'MOSI'                                                # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'SOLO_CAM'       \n",
        "classNo                 = 7                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.8                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 200                                                   # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)   # hier ersetzen\n",
        "MHCA_withoutMHA(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Study: Without CAM Module, Solo MHA"
      ],
      "metadata": {
        "id": "0jlpYYJLjfs1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q7Pt-rsHR1w"
      },
      "outputs": [],
      "source": [
        "def MHCA_withoutCAM(dataset,classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model but without the CAM module. \n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys  = 'text_audio_video' \n",
        "    runs = 1\n",
        "    acc = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\t\t # Shape of Modalitys\n",
        "        text_shape     = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape    = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape    = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "\n",
        "        # ============ Multi-Head Attention (MHA) Module  ======================\n",
        "        text_MHA       = MultiHeadAttention(head_num =5)(text_shape)            # All (5)\n",
        "        audio_MHA      = MultiHeadAttention(head_num =2)(audio_shape)           # MOSI (5), YOUTUBE, MOUD, MMMO (2)\n",
        "        video_MHA      = MultiHeadAttention(head_num =7)(video_shape)           # MOSI (5), YOUTUBE, MOUD, MMMO (7)\n",
        "\n",
        "\n",
        "\t    \t# ============ Concatenation ===========================================\t\n",
        "        merged          = concatenate([text_MHA,audio_MHA,video_MHA])\n",
        "        merged          = Dense(neurons, activation='relu')(merged)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model           = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                                outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        # plot_model(model, to_file='MHA_and_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + dataset + str(modalitys)+'_'+str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train_text,train_audio,train_video], \n",
        "                                    [train_audio,train_video,train_text,train_label],\n",
        "                                    epochs=50,\n",
        "                                    batch_size=16,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                    [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                    verbose=1)\n",
        "\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test_text,test_audio,test_video])\n",
        "        result[-1]      = np.nan_to_num(result[-1])\n",
        "        #model.summary()\n",
        "        acc, f1_score   = calculate_accuracy(result[-1], test_label, True)\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "        open('/content/drive/MyDrive/results/'+ dataset + '_' + modelType + '_'+ modalitys+'.txt', 'a').write(str(acc) + '\\n'*2)  \n",
        "\n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "  \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()   \n",
        "        %load_ext autotime\n",
        "\n",
        "dataset                 = 'YOUTUBE'                                             # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'SOLO_MHA'       \n",
        "classNo                 = 3                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.3                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 50                                                    # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)\n",
        "MHCA_withoutCAM(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVWWHE-I1Wix"
      },
      "source": [
        "### Reversed-Order: Dense + CAM + MHA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MHCA_Order(dataset,classNo, drop, neurons):\n",
        "    \"\"\"\n",
        "        This function implements the MHCA model but changes the order of the MHA and CAM module.  \n",
        "        The inputs are: the selected dataset (dataset), the number of classes of the dataset (classNo), the dropout rate\n",
        "        in the dense layer (drop), and the number of neurons in the dense layer (neurons).\n",
        "    \"\"\"\n",
        "    modalitys  = 'text_audio_video' \n",
        "    runs = 1\n",
        "    acc = 0\n",
        "\n",
        "    for run in range(runs):\n",
        "\t\t    # Shape of Modalitys\n",
        "        text_shape     = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "        audio_shape    = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "        video_shape    = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "        # ============ Dense Module ============================================\n",
        "        text_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(text_shape))\n",
        "        audio_MHA_fc   = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_shape))\n",
        "        video_MHA_fc   = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_shape))\n",
        "  \n",
        "        # ============  Context-aware Attention (CAM) Module ===================\n",
        "        cam_TA          = CAM(text_MHA_fc, audio_MHA_fc)\n",
        "        cam_TV          = CAM(text_MHA_fc, video_MHA_fc)\n",
        "        cam_AV          = CAM(audio_MHA_fc, video_MHA_fc)\n",
        "\n",
        "        # ============ Multi-Head Attention (MHA) Module  ======================\n",
        "        text_MHA        = MultiHeadAttention(head_num =5)(cam_TA)\n",
        "        audio_MHA       = MultiHeadAttention(head_num =5)(cam_TV)\n",
        "        video_MHA       = MultiHeadAttention(head_num =5)(cam_AV)\n",
        "\n",
        "\n",
        "    \t\t# ============ Concatenation ===========================================\t\n",
        "        merged          = concatenate([text_MHA,audio_MHA,video_MHA])\n",
        "        merged          = Dense(neurons, activation='relu')(merged)\n",
        "        merged          = MeanOverTime()(merged)\n",
        "        final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "        # ============ Model ===================================================\n",
        "\n",
        "        model           = Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "                                outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "\n",
        "        model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "        plot_model(model, to_file='MHA_Dense_CAM.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "        path            = '/content/drive/MyDrive/weights/' + dataset + str(modalitys)+'_'+str(run)+'.hdf5'\n",
        "        check1          = EarlyStopping(monitor='val_final_output_loss', patience=20)\n",
        "        check2          = ModelCheckpoint(path, monitor='val_final_output_acc', verbose=1, save_weights_only=True,  save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "        history         = model.fit([train_text,train_audio,train_video], \n",
        "                                    [train_audio,train_video,train_text,train_label],\n",
        "                                    epochs=50,\n",
        "                                    batch_size=16,\n",
        "                                    shuffle=True,\n",
        "                                    callbacks=[check1, check2],\n",
        "                                    validation_data=([valid_text,valid_audio,valid_video], \n",
        "                                    [valid_audio,valid_video,valid_text,valid_label]),\n",
        "                                    verbose=1)\n",
        "\n",
        "        model.load_weights(path)\n",
        "        result          = model.predict([test_text,test_audio,test_video])\n",
        "        result[-1]      = np.nan_to_num(result[-1])\n",
        "\n",
        "        #model.summary()\n",
        "        acc, f1_score = calculate_accuracy(result[-1], test_label, True)\n",
        "\n",
        "        print('Best accuracy is: ', acc)\n",
        "        print('F1 Score is: ', f1_score)\n",
        "\n",
        "        open('/content/drive/MyDrive/results/'+ dataset + '_' + modelType + '_'+ modalitys+'.txt', 'a').write(modalitys + ', accuracy: ' + str(acc) + '\\n'*2)  \n",
        "\n",
        "\n",
        "       # ============ Plot Accuracy & Loss History =============================\n",
        "  \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "        ax1.plot(history.history['final_output_acc'])\n",
        "        ax1.plot(history.history['val_final_output_acc'])\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "        ax2.plot(history.history['final_output_loss'])\n",
        "        ax2.plot(history.history['val_final_output_loss'])\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.show()     \n",
        "        %load_ext autotime\n",
        "\n",
        "dataset                 = 'MOSI'                                                # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'Order_CAM_Dense_MHA'       \n",
        "classNo                 = 7                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.8                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 200                                                   # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "featuresExtraction(dataset, classNo)   # hier ersetzen\n",
        "MHCA_Order(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons)"
      ],
      "metadata": {
        "id": "cHylP0B3i0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Weights or Results\n"
      ],
      "metadata": {
        "id": "SyNI5rNxl2cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Load_weights_history(dataset,classNo, drop, neurons, weights_path, history_path):\n",
        "\n",
        "  acc = 0\n",
        "\n",
        "\t# Shape of Modalitys\n",
        "  text_shape      = Input(shape=(train_text.shape[1], train_text.shape[2]))\n",
        "  audio_shape     = Input(shape=(train_audio.shape[1], train_audio.shape[2]))\n",
        "  video_shape     = Input(shape=(train_video.shape[1], train_video.shape[2]))\n",
        "\n",
        "\n",
        "\t# ============ Multi-Head Attention (MHA) Module  ======================\n",
        "  text_MHA        = MultiHeadAttention(head_num =5)(text_shape)           # All (5)\n",
        "  audio_MHA       = MultiHeadAttention(head_num =5)(audio_shape)          # MOSI (5), YOUTUBE, MOUD, MMMO (2)\n",
        "  video_MHA       = MultiHeadAttention(head_num =5)(video_shape)          # MOSI (5), YOUTUBE, MOUD, MMMO (7)\n",
        "\n",
        "\t# ============ Dense Module ============================================\n",
        "  text_MHA_fc     = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh' ))(text_MHA))\n",
        "  audio_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(audio_MHA))\n",
        "  video_MHA_fc    = Dropout(drop)(TimeDistributed(Dense(neurons, activation='tanh'))(video_MHA))\n",
        "\n",
        "\t# ============  Context-aware Attention (CAM) Module ===================\n",
        "  cam_TA          = CAM(text_MHA_fc, audio_MHA_fc)\n",
        "  cam_TV          = CAM(text_MHA_fc, video_MHA_fc)\n",
        "  cam_AV          = CAM(audio_MHA_fc, video_MHA_fc)\n",
        "\n",
        "\t# ============ Concatenation ===========================================\t\n",
        "\n",
        "  merged          = concatenate([cam_TA,cam_TV,cam_AV])\n",
        "  merged          = Dense(neurons, activation='relu')(merged)\n",
        "  merged          = MeanOverTime()(merged)\n",
        "  final_output    = Dense(classNo, activation='softmax', name='final_output')(merged)   # hier anpassen wieviele outputs man hat!!\n",
        "\n",
        "\t# ============ Model ===================================================\n",
        "\n",
        "  model \t\t\t= Model(inputs=[text_shape,audio_shape,video_shape],\n",
        "\t\t\t\t  outputs=[audio_shape,video_shape,text_shape,final_output])\n",
        "  model.compile(loss=['mse','mse','mse','categorical_crossentropy'], sample_weight_mode='None', optimizer='adam', metrics=['acc'])\n",
        "  \n",
        "  model.load_weights(weights_path)\n",
        "  result \t        = model.predict([test_text,test_audio,test_video])\n",
        "  result[-1]      = np.nan_to_num( result[-1])\n",
        "  test_label_     = [array.tolist() for array in test_label]\n",
        "  acc, f1_score  = calculate_accuracy(result[-1], test_label_, True)\n",
        "\n",
        "  print('Best accuracy is: ', acc)\n",
        "  print('F1 Score is: ', f1_score)\n",
        "\n",
        "  with open(history_path, 'rb') as file:\n",
        "    history = pickle.load(file)\n",
        "\n",
        "  fig, (ax1, ax2)     = plt.subplots(1, 2, figsize=(15,5))\n",
        "  ax1.plot(history['final_output_acc'])\n",
        "  ax1.plot(history['val_final_output_acc'])\n",
        "  ax1.set_title('Model Accuracy')\n",
        "  ax1.set_ylabel('Accuracy')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.legend(['Training', 'Validation'], loc='lower right')\n",
        "  ax2.plot(history['final_output_loss'])\n",
        "  ax2.plot(history['val_final_output_loss'])\n",
        "  ax2.set_title('Model Loss')\n",
        "  ax2.set_ylabel('Loss')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "  plt.show()    \n",
        "\n",
        "dataset                 = 'MOSI'                                             # Datasets: MOSI, YOUTUBE, MOUD, MMMO\n",
        "modelType               = 'MHA_Dense_CAM'       \n",
        "classNo                 = 2                                                     # MOSI (2,7), YOUTUBE (3), MOUD (2), MMMO (2)\n",
        "drop                    = 0.8                                                   # MOSI (0.8), YOUTUBE (0.3), MOUD & MMMO (0.9)\n",
        "neurons                 = 200                                                    # MOSI (200), YOUTUBE (50), MOUD & MMMO (100)\n",
        "weights_path            = '/content/14_59_MOSI_text_audio_video_0.8_200_2.hdf5'\n",
        "history_path            = '/content/14_59MOSI2_MHA_Dense_CAM_0.8_200_2_history.pkl'\n",
        "featuresExtraction(dataset, classNo)\n",
        "%memit (Load_weights_history(dataset=dataset, classNo=classNo, drop=drop, neurons=neurons, weights_path=weights_path, history_path=history_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "5_jSwuEHobyl",
        "outputId": "54812c0e-f9f2-4f0b-f2cb-f682a74f9fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 5ms/step\n",
            "Confusion Matrix :\n",
            "[[329  50]\n",
            " [ 89 218]]\n",
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.87      0.83       379\n",
            "           1       0.81      0.71      0.76       307\n",
            "\n",
            "    accuracy                           0.80       686\n",
            "   macro avg       0.80      0.79      0.79       686\n",
            "weighted avg       0.80      0.80      0.80       686\n",
            "\n",
            "Best accuracy is:  0.7973760932944607\n",
            "F1 Score is:  0.79929\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAFNCAYAAABfWL0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3iW9dnG8e+VTSAJK+y9p4CgKKKCKCjWPSo4wN06WuvoeLVabX21Vdu6rfXFWbGKCwEHQwXFAahA2GHvMEOAQNb1/nE/kRhGAuTJk3F+jiNH8tzzDIdyc92/Ze6OiIiIiIiIVF1RkQ4gIiIiIiIi4aXCT0REREREpIpT4SciIiIiIlLFqfATERERERGp4lT4iYiIiIiIVHEq/ERERERERKo4FX4iYWJmrczMzSymFMeONLMvyiOXiIhIZaVnq8iRU+EnApjZCjPLMbP6xbZ/H3rAtIpMsp9kqWVmO83sw0hnERERKUlFfrYeTgEpUlWo8BPZZzkwrPCDmXUHEiMXZz8XAXuBM8ysUXneWA9GERE5QhX92SpSbajwE9nnVeCqIp9HAK8UPcDMUszsFTPbZGYrzeweM4sK7Ys2s0fNbLOZLQPOPsC5/2dm681srZn9xcyiDyPfCOA5YA5wRbFr9zez6Wa23cxWm9nI0PYaZvZYKGummX0R2jbAzNYUu8YKMzs99POfzGyMmb1mZjuAkWZ2vJl9FbrHejN7ysziipzf1cwmmtlWM9toZv9jZo3MbLeZ1Sty3LGhP7/Yw/jdRUSkcqroz9b9mFkTMxsbep6lm9n1RfYdb2YzzWxH6Fn399D2hNAzc0voOTnDzBoeTQ6RsqbCT2Sfr4FkM+scemhcBrxW7JgngRSgDXAqwcPs6tC+64GfAb2APsDFxc59CcgD2oWOGQxcV5pgZtYSGAD8J/R1VbF9H4aypQI9gR9Cux8FegP9gLrAb4GC0twTOA8YA9QO3TMf+A1QHzgRGATcFMqQBEwCPgKahH7Hye6+AfgMuLTIda8E3nD33FLmEBGRyqvCPlsP4Q1gDcHz7GLgf83stNC+x4HH3T0ZaAu8Gdo+IvQ7NAfqAb8Aso8yh0iZUuEn8lOFbybPABYAawt3FHlg/cHds9x9BfAYQSEDQXHzT3df7e5bgYeKnNsQGArc5u673D0D+EfoeqVxJTDH3ecTPJC6mlmv0L7hwCR3H+3uue6+xd1/CL0tvQb4tbuvdfd8d5/u7ntLec+v3P09dy9w92x3n+XuX7t7Xuh3/xfBAxqCh/IGd3/M3feE/ny+Ce17mVALZejPcBjBn7OIiFQPFfXZuh8zaw6cBPwu9Dz7AXiBfS9cc4F2Zlbf3Xe6+9dFttcD2oWet7PcfceR5hAJB43bEfmpV4GpQGuKdUUhaOmKBVYW2bYSaBr6uQmwuti+Qi1D5643s8JtUcWOP5SrgH8DuPtaM/uc4O3i9wRvF5ce4Jz6QMJB9pXGT7KZWQfg7wRvXBMJ/v6YFdp9sAwA7wPPmVlroCOQ6e7fHmEmERGpfCrqs/VAmgBb3T2r2D37hH6+FngAWGhmy4H73X0cwe/YHHjDzGoTtGrerd4tUpGoxU+kCHdfSTAQfSjwTrHdmwne6LUssq0F+95crif4S7/ovkKrCSZmqe/utUNfye7etaRMZtYPaA/8wcw2mNkGoC8wPDTpymqC7ibFbQb2HGTfLooMrg+9cU0tdowX+/wssBBoH+ri8j9A4ZN2NUEXnf24+x6CrjBXELzBVWufiEg1UhGfrYewDqgbGsKwXx53X+Luw4AGwF+BMWZWM9Tj5n5370IwvOJn/HRso0jEqfAT2d+1wGnuvqvoRnfPJyhgHjSzpNDYutvZN1bhTeBXZtbMzOoAvy9y7nrgE+AxM0s2sygza2tmp1KyEcBEoAvB+L2eQDegBnAWwfi7083sUjOLMbN6ZtbT3QuAUcDfQwPVo83sRDOLBxYDCWZ2dmiSlXuA+BJyJAE7gJ1m1gn4ZZF944DGZnabmcWH/nz6Ftn/CjASOBcVfiIi1VFFe7YWig9NzJJgZgkEBd504KHQtmNC2V8DMLMrzCw19IzdHrpGgZkNNLPuoRepOwiK2dKOqRcpFyr8RIpx96XuPvMgu28laC1bBnwBvE5QXEHQFfNjYDbwHfu/1bwKiAPmA9sIJk5pfKgsoYfQpcCT7r6hyNdyggJqhLuvIniLegewlWBilx6hS9wJzAVmhPb9FYhy90yCiVleIHjI7SIYyH4odxKMJ8wK/a7/LdwR6hJzBnAOsAFYAgwssv9Lggfgd6E3vyIiUo1UpGdrMTsJJmEp/DqNYCx6K4LWv3eB+9x9Uuj4M4F5ZraTYKKXy9w9G2gUuvcOgnGMn6MXnVLBmHvx3lwiImXPzKYAr7v7C5HOIiIiIlLdqPATkbAzs+MIuqs2LzZgXkRERETKgbp6ikhYmdnLBGv83aaiT0RERCQy1OInIiIiIiJSxanFT0REREREpIpT4SciIiIiIlLFxUQ6QFmpX7++t2rVKtIxRESkHMyaNWuzu6dGOkdloWekiEj1cKjnY5Up/Fq1asXMmQdbHkZERKoSM9N6kIdBz0gRkerhUM9HdfUUERERERGp4sJW+JnZKDPLMLO0g+w3M3vCzNLNbI6ZHVtk3wgzWxL6GhGujCIiIiIiItVBOFv8XgLOPMT+s4D2oa8bgGcBzKwucB/QFzgeuM/M6oQxp4iIiIiISJUWtsLP3acCWw9xyHnAKx74GqhtZo2BIcBEd9/q7tuAiRy6gBQREakSSuotEzpmgJn9YGbzzOzz8swnIiKVVyTH+DUFVhf5vCa07WDbRUREqrqXOMTLTjOrDTwDnOvuXYFLyimXiIhUcpV6chczu8HMZprZzE2bNkU6joiIyFEpRW+Z4cA77r4qdHxGuQQTEZFKL5KF31qgeZHPzULbDrZ9P+7+vLv3cfc+qalazklERKq8DkAdM/vMzGaZ2VWRDiQiIpVDJAu/scBVodk9TwAy3X098DEw2MzqhCZ1GRzaJiIiUt3FAL2BswnGxP/RzDoc6ED1ihERkaLCtoC7mY0GBgD1zWwNwUydsQDu/hwwARgKpAO7gatD+7aa2Z+BGaFLPeDuh+r2IiIiUl2sAba4+y5gl5lNBXoAi4sf6O7PA88D9OnTx8s1pYiIVDhhK/zcfVgJ+x24+SD7RgGjwpFLREQOn7uzcstu5q3bQU5+PskJsaTU2PeVXCOWhNjoSMesDt4HnjKzGCCOYOmjf0Q2koiIHLWN82DzYuh6QdhuEbbCT0REKqeCAmf5ll2krc0kbW0mc9dmMm/dDrL25B3yvPiYqJ8UgsULw8Kf+7auS/O6ieX021QuJfWWcfcFZvYRMAcoAF5w94Mu/SAiIpXAnkz47xWQsxvanQHxtcJyGxV+IiLVWH6Bs2zTTtLWZTJ3zQ7S1mYyb10mu3LyAYiLiaJzoyTO7dGE7k1T6NokhVoJMWRm5/7ka0ex75nZuWzcsYfFG7PIzM79SdH4z5/3VOF3ECX1lgkd8wjwSDnEERGRcHOH926C7atgxLiwFX2gwk9EpEQFBU7W3ryfFDd1asbRsWESUVEW6Xillp2Tz9JNO1m4IevHlrz563aQnRsUeQmxUXRunMxFvZvRrWkK3Zqk0L5hLWKjj34esPwCZ+eePDKzc6ldM/aoryciIlIlfPk4LBwHQx6ClieG9VYq/ESk2tm+O4e0tTvYnp1zkFarvJ9sz9qTS8EBpsaoWzOOfm3rcVK7+vRvV7/CtGLt2pvH0k07WbJxJ0sydpKekcXijTtZvW03Hvo9EuOi6dI4mZ8f15zuTVPo1jSFtqk1iSmDIu9AoqOMlMRYUhJV9ImIiACwfCpMvj8Y13fCL8N+OxV+IlLluTvpGTuZvDCDKQszmLVyG/nFKrm46KjQOLQYUmrEUr9WHG1Sax5wnFpyQizrtmfzZfpmvly6mXFz1gPQvG4N+rerT7+29enXth71asWH9ffauTePJRuzQsXdzh9/XrMt+8djYqONNvVr0b1ZChce25QODZPo0LAWrevXIroStVaKiIhUKTvWwZhroF47OPdJsPA/k1X4iUiVtDcvn2+WbWXKwgwmL9zI6q1BMdSlcTI3DWjLiW2CwqywqEuIjcIO8y/di3o3w91ZumknX6Zv4Yv0oAgc/e1qADo3TqZ/u3r0a1ef41vVpWZ86f/KzckrYNPOvWTs2ENGVtHve1mXmc3SjJ2sy9zz4/FxMVG0Ta3FsS3q8PM+zWnfsBbtGybRsm5i2FrxRERE5Ajk5cCbI4LJXEaOh/ikcrmtCj8RqTIyduzh00VBq960JZvZnZNPfEwU/dvV5xentuW0Tg1onFKjTO9pZrRrkES7BkmM6NeKvPwC5q7NZPrSLXyxZDMvT1/Jv6ctJzba6NW8Die1q8+JbesRHQUZO/aysbCgywp+3hT6eeuunP3uFWVQv1Y8DZMT6NumHu0a1KJ9g6DAa1E3US14IiIilcHEP8Kab+HiFyG1Y7ndVoWfiFRaBQXOvHU7mLxwI1MWZjBnTSYATVISuPDYppzWqQEntqlPjbjyW18uJjqKXi3q0KtFHW4e2I7snHxmrtzKl+lb+DJ9M/+cvJh/TCp2TpSRmhRPg6R4mtdNpHfLOjRISqBBcjwNk+ODn5PiqVcrXsWdiIhIZTZ3DHzzHJxwE3S7sFxvrcJPRCqdOWu2M/rbVUxekEFG1l7MoFfz2tw1pCOndWpAp0ZJh91tM1xqxEVzcvtUTm6fCgQTy8xcsY2YaPuxuKubGFepZgcVERGRI5CxAMbeCs1PgDMeKPfbq/ATkUohJ6+AD9PW8/L0FXy3ajuJcdEM7NiA0zo1YEDH1LBPpFJWaifGcXqXhpGOISIiIuVpzw7475UQVwsueQmiy3+WaxV+IlKhbcray+vfrOI/36wkI2svreolct85Xbi4dzOSErQ0gIiIiFRw7vD+zbB1GYwYC8mNIxJDhZ+IVEizV2/n5ekrGDdnPTn5BZzaIZW/XtyKU9unqlukiIiIVB5fPQ0LxsIZf4ZW/SMWQ4WfSCW1Ny+fj9I20KR2DXo1r10lpuwv7M750vQVfL9qOzXjohnetwVXntiStqm1Ih1PRERE5PCs+BIm3gudz4F+t0Y0igo/kUrG3Rk/dz0Pf7jwx4W6ayfGMqBDKqd1bsip7VNJSaxcXSALu3O+9s1KNmXtpXX9murOKSIiIpVb1gYYczXUbQ3nPVMui7Qfigo/kUpk1sptPDh+Pt+t2k7nxsm8eHU3snPymbwgg08XZfDeD+uIjjL6tKzDaZ0aMKhzA9qm1iqTGS5z8gpYvDGLtLWZpK3LZO7aHWzdtZfkhFiSE2J/XAg9JTH4nhz6nJwQs29faHtsqHVy9urtvDR9BePmrCM33zm1Qyoj1Z1TREREKrv8XHhrJOzNgivfg4TkSCdS4SdSGazeupu/frSQcXPW0yApnr9dfAwXHdvsxzXdhnZvTH6BM3vNdqYsyGDywgwe+nAhD324kBZ1E38sAo9vXZf4mJLXtNuTm8/ijVnMXZtJ2todpK3NZNGGLHLyCwBIio+ha9NkereoQ9aePDKzc1m6aSeZ2blkZueyN6/gkNevGRdNYnwMm7L2UjMumsv7tlR3ThEREak6Jv0JVn0FF74ADbtEOg2gwk+kQsvMzuWZT9N58csVREXBrwa158ZT2lAzfv//daOjjGNb1OHYFnW4c0hH1m3PZsrCDKYszGD0t6t4afoKaobWlDutcwMGdmxAalI8e3LzWbA+KO4KC73FG7PIK3AAUmrE0q1pMlf3b0X3pil0a5JCi7qJh2yR25Obz45QEVj4tWNPLpm7c8nMzvvxc9cmyerOKSIiIlXLvPfgq6fg+BvgmEsineZHKvxEKqDc/AJGf7uKf0xczPbsXC46thl3Du5Io5SEUl+jSe0aXHFCS644oSXZOflMX7qZyQszmLIgg4/mbQCged0arNu+h/xQkVcnMZZuTVO4oWMbujVNoXvTFJrVqXHYXUUTYqNJiI2mQXLp84qIiIhUepsWB0s3NDsOBj8Y6TQ/ocJPpAJxd6YszODBCQtYtmkXJ7apx91nd6Zb05Sjum6NuGgGdW7IoM4N8fOd+et3MGVBBvPX7+D8nk3p2iSF7s1SaJKSUCbjAUVERESqnb074b9XQEwCXPIyxMRFOtFPqPATqSDmrcvkwfELmL50C23q1+TfV/Xh9M4NyrwQMzO6Nkmha5OjKyZFREREJMQdPvgVbFkSTOaS0jTSifajwk8kwjZk7uHRTxbx9ndrqF0jlvvP7crwvi1+nPlSRERERCqwvBz48LeQ9jYMug/anBrpRAekwk+knG3euZclG3eSnpHF/PVZvPf9WvILnOtPbsPNA9uRUkMTnYiIiIhUCrs2w5sjYOUX0P83cNJtkU50UCr8RMLA3dm0cy/pG3eyeGMWSzJ2siRjJ+kZO9m6K+fH42rFxzCocwN+O6QTLeolRjCxiIiIiByWDWnwxjDYmREs21CBZvA8EBV+IkdpQ+YelmRksWTjziLfgzXtCiUlxNChYRKDuzSkfcMk2jeoRfuGtWiUrMlURERERCqdBePgnRuChdmvngBNe0c6UYnCWviZ2ZnA40A08IK7P1xsf0tgFJAKbAWucPc1oX35wNzQoavc/dxwZhU5XDNWbOWRjxbx7YqtP26rnRhLhwZJnH1MYzo0qPVjkZeaFK8CT0RERKSyc4epj8KnfwmKvZ//B5IbRzpVqYSt8DOzaOBp4AxgDTDDzMa6+/wihz0KvOLuL5vZacBDwJWhfdnu3jNc+USO1Lx1mTz68SI+XbSJBknx/P6sTvRoVpv2DWtRr2acCjwROWJmNgr4GZDh7t0OcdxxwFfAZe4+przyiYhUazm74f2bYN670P1SOPcJiK0R6VSlFs4Wv+OBdHdfBmBmbwDnAUULvy7A7aGfPwXeC2MekaOyfPMu/j5xMR/MXkdKjVh+f1YnRpzYihpx0ZGOJiJVx0vAU8ArBzsg9GL1r8An5ZRJREQy18Abw2H9HDj9fjjp11DJXvaHs/BrCqwu8nkN0LfYMbOBCwm6g14AJJlZPXffAiSY2UwgD3jY3VUUSkSsz8zmiclLeHPmGuKio7hlYDuuP6WNZt8UkTLn7lPNrFUJh90KvA0cF/ZAIiICq7+FNy6H3GwY/l/oMCTSiY5IpCd3uRN4ysxGAlOBtUB+aF9Ld19rZm2AKWY2192XFj3ZzG4AbgBo0aJF+aWWamHbrhye+Sydl79aibtz5QktuXlgO1KT4iMdTUSqKTNrSvCidCAq/EREwu/7/8C42yC5KYz4ABp0inSiIxbOwm8t0LzI52ahbT9y93UELX6YWS3gInffHtq3NvR9mZl9BvQClhY7/3ngeYA+ffp4WH4LqXZ27s3j/6Yt59/TlrE7J48LejXjttPb07yullsQkYj7J/A7dy8oaTyxXo6KiByF/DyYdB989RS0PhUueQkS60Y61VEJZ+E3A2hvZq0JCr7LgOFFDzCz+sBWdy8A/kAwwydmVgfY7e57Q8ecBPwtjFlF2JObz3++WcUzn6azZVcOQ7o25M7BHWnfMCnS0URECvUB3ggVffWBoWaWd6DhEHo5KiJyhLK3w5hrYOlkOP5GGPIgRFf+IT5hK/zcPc/MbgE+JljOYZS7zzOzB4CZ7j4WGAA8ZGZO0NXz5tDpnYF/mVkBEEUwxm/+fjcRKQN5+QW8891a/jlpMesy93BSu3rcNaQTPZvXjnQ0EZGfcPfWhT+b2UvAOI2BFxEpQ5vTYfRlsG05nPM49B4Z6URlJqxj/Nx9AjCh2LZ7i/w8BthvGmp3nw50D2c2qb5y8wtYsnEnaWszSVuXybQlm1m+eRc9mtfmkUt6cFK7+pGOKCLVlJmNJngpWt/M1gD3AbEA7v5cBKOJiFR96ZPhrashOgauGgutTop0ojIV6cldRMIqJ6+AxRuzmLs2Myj01mayYEMWOXkFANSMi6Zb0xR+d2YnhnRtqDX4RCSi3H3YYRw7MoxRRESql11bYPQwqNcOho2GOi0jnajMqfCTKmNPbj6LNmSRti4o8OauzWTRhixy84OhLUnxMXRtmsyIE1vSrWkK3Zqm0LpeTaKiVOyJiIiIVGuLP4L8vXD+01Wy6AMVflJJuTvLN+9i5sptfLdyG7PXZLJkYxZ5BUGRl1Ijlm5Nk7mmf2u6NUmhe9MUWtRNVJEnIiIiIvtbOA6Sm0HjnpFOEjYq/KRS2JuXT9raTGau2PZjsbdlVw4AyQkx9Ghem4Ed29CtaVDkNatTQ902RURERKRkObtg6RQ4dgRU4X8/qvCTCmnrrhxmrdzGzJVbmbViG3PWZv44Lq9VvUQGdGxAn1Z16NOyDm1Ta6klT0RERESOTPpkyNsDnX8W6SRhpcJPIq5ot81ZK4Jib+mmXQDERhvdmqYw4sSW9G5Zl94t65CaFB/hxCIiIiJSZSwcBzXqQIt+kU4SVir8JKL25Obzi9dm8dmiTQDUToyld4s6XHhsM/q0rEOP5rVJiI2OcEoRERERqZLyc4OJXTqeHSzjUIVV7d9OKrTc/AJuHf09ny3axJ2DOzCkayN12xQRERGR8rPiC9iTWeW7eYIKP4mQ/ALnzrdmM3H+Ru4/tysj+rWKdCQRERERqW4WjoeYGtBmYKSThF1UpANI9ePu3PNeGu//sI67hnRU0SciIiIi5a+gICj82g2CuMRIpwk7FX5Srtyd/52wgNHfruKmAW25eWC7SEcSERERkepo/feQtQ46Vf1unqDCT8rZ45OX8O9pyxnZrxV3DekY6TgiIiIiUl0tGAcWDR2GRDpJuVDhJ+XmhWnL+OekJVzcuxn3/qyLFlgXERERkchZOB5anQSJdSOdpFyo8JNy8fo3q/jL+AUM7d6Ihy/srpk7RURERCRyNi+BzYug0zmRTlJuVPhJ2L3/w1rufm8uAzum8s+f9yImWv/ZiYiIiEgELRwXfO80NLI5ypH+BS5h9cm8Ddz+5mz6tq7Ls1f0Ji5G/8mJiIiISIQtGAdNekFKs0gnKTf6V7iEzbQlm7jl9e/p3jSFF0YcR0JsdKQjiYiIiEh1t2M9rJ0Jnc6OdJJypcJPwmLGiq3c8Mos2qTW5KWrj6NWfEykI4mIiIiIwKLxwfdqNL4PVPhJGMxdk8k1L86gcUoCr17bl9qJcZGOJCIiIiISWDge6rWD1Oq1tJgKPylTSzZmcdWob0iuEctr1/UlNSk+0pFERERERALZ22H51KCbZzVbWkyFn5SZlVt2cfkL3xATHcV/rutLk9o1Ih1JRERERGSfJROhIK/adfMEFX5SRtZnZjP839+Qm1/Af67rS6v6NSMdSURERETkpxZ+ALUaQdPekU5S7lT4yVFbtz2by1/4hszsXF65pi8dGiZFOpKIiIiIyE/lZsOSScHafVHVrwyqfr+xlJn8AufFL5dzxt8/Z0PmHkaNPI7uzVIiHUtEREREKht3mPUyvH8z5OWE5x7LPofcXdVuGYdCYS38zOxMM1tkZulm9vsD7G9pZpPNbI6ZfWZmzYrsG2FmS0JfI8KZUw5f2tpMLnjmS+7/YD59WtXl49tO4fjWdSMdS0SkUjOzUWaWYWZpB9l/eeiZOdfMpptZj/LOKCJS5rI2wOuXwge/gu9fg2//FZ77LPwA4pOh1SnhuX4FF7bF1cwsGngaOANYA8wws7HuPr/IYY8Cr7j7y2Z2GvAQcKWZ1QXuA/oADswKnbstXHmldHbtzeMfExcz6svl1K0Zz1PDe3F298ZYNZsVSUQkTF4CngJeOcj+5cCp7r7NzM4Cngf6llM2EZGyl/YOjL8dcvfAWX+D9Enw2cPQ/RJIalR29ynIh0UfQvvBEFM9lxoLZ4vf8UC6uy9z9xzgDeC8Ysd0AaaEfv60yP4hwER33xoq9iYCZ4Yxq5TCpPkbOePvn/PCF8sZdnwLJt9xKj87pomKPhGRMuLuU4Gth9g/vchL0K+BZgc7VkSkQtu9FcZcC2Ouhrpt4BfToO+NcObDkJ8DE+8r2/ut+hp2b4HOPyvb61YiYWvxA5oCq4t8XsP+byVnAxcCjwMXAElmVu8g5zYNX1Q5lA2Ze/jT2Hl8NG8DHRsm8fbwXvRuqW6dIiIRdi3wYaRDiIgctvTJwVi+XZtg4N3Q/3aIDpUl9dpCv1th2mPQeyS0PLFs7rlwPETHQ7vTy+Z6lVA4C7/SuBN4ysxGAlOBtUB+aU82sxuAGwBatGgRjnzVWn6B89rXK3nk40Xk5hfw2zM7cv3JbYiN1pxAIiKRZGYDCQq//oc4Rs9IEalYcnbBxHthxgtQvyMMGw1Neu1/3Ml3wOw34MO74IbPISr66O7rHozvazMA4qvv7PPh/Bf8WqB5kc/NQtt+5O7r3P1Cd+8F3B3atr0054aOfd7d+7h7n9TU1LLOX63NW5fJhc9O576x8+jVojYTf3MqNw1op6JPRCTCzOwY4AXgPHffcrDj9IwUkQpl9bfwXH+Y8X9w4i1w4+cHLvoA4mrC4L/Ahrkw68Wjv/fGNNi+qtrO5lkonC1+M4D2ZtaaoGi7DBhe9AAzqw9sdfcC4A/AqNCuj4H/NbM6oc+DQ/slzHbn5PHPSUv4vy+WUycxlieG9eKcYzR5i4hIRWBmLYB3gCvdfXGk84iIlCgvBz5/GL74ByQ3hREfQOuTSz6v6wUwcxRM/jN0uQBq1jvyDAvGgUVBx6FHfo0qIGyFn7vnmdktBEVcNDDK3eeZ2QPATHcfCwwAHjIzJ+jqeXPo3K1m9meC4hHgAXc/6GB3KRufLszgnvfSWLs9m2HHt+D3Z3YiJTE20rFERKoNMxtN8Gysb2ZrCGa4jgVw9+eAe4F6wDOhF3J57t4nMmlFREqwcR68cyNsnAs9r4AzH4KE5NKdawZDH4FnT4IpD8A5jx95joXjofkJUKt6934I6xg/d58ATCi27d4iP48Bxhzk3FHsawGUMNqbl88db85m3Jz1tG9QizG/OJE+rTR5i4hIeXP3YSXsvw64rpziiIgcmYJ8+OopmPIXSEiBy0ZDpyNobWvQOZjp8+tng4leDtY19FC2rQgKz5REddkAACAASURBVMEPHv65VUykJ3eRCuCJyUsYN2c9vzm9A78c0Ja4GI3jExEREZEjsHU5vPdLWPUVdPpZ0FJXs/6RX2/A72HuWzDhLrjmE4g6zH+nLhwffK/m4/tAhV+198Pq7Tz72VIu7dOMX5/ePtJxRERERKSy2bYyWHg9fTIsnQLRsXD+c9DjsqDL5tFISIEzHgiKyTlvQM/hJZ9T1IJx0LAb1G19dDmqABV+1die3HzuePMHGiUncM/PukQ6joiIiIhUBrnZsPJLWDIpKPi2LAm2p7QICrP+v4HazQ99jcNxzGXBRC8T7w1a7hJSSnfezk2w+ms45bdll6USU+FXjf194mKWbtrFq9ceT3KCJnERERERkQNwhy3poVa9SbDiC8jbEyyI3qo/HHdtsDB6vXZH38J3IFFRwUQvzw+Ezx4OJokpjcUfgheom2eICr9qauaKrfx72jIu79uCk9tX7xmORERERKSYvVmwfNq+Ym/7ymB7vfbQ++qg0GvZD+ISyydPk17BBC/f/AuOvSqY+KUkC8ZB7RbQqHvY41UGKvyqoeycfO58azZNa9fgD0NL8T+NiIiIiFQPae8E3SpXfQ0FuRBbE9qcCif9CtoOiuxYuUH3wrx3g4leRnxw6NbFvVmw7LOgNVLrUQMq/Kqlv360kBVbdjP6+hOoFa//BEREREQE2LUZ3r4uaCU74ZdBq16LEyAmPtLJAol1YdAfYfwdQQHY7cKDH5s+CfL3BjOLCqDCr9r5aukWXpq+gpH9WnFi23qRjiMiIiIiFcX898Hz4eevVtzukb2vhlkvwSf3QIchEFfzwMctHA+J9YLCVQDQgm3VyK69edw1Zjat6iXy2zM7RjqOiIiIiFQkaW9D/Y7B8gcVVVQ0DH0UdqyFaY8d+Ji8HFj8CXQ8KzheABV+1cr/TljA2u3ZPHpJDxLj1NgrIiIiIiGZa2HldOh+ccUfE9fihGCJh+lPwpal++9fMQ32ZqqbZzEq/KqJaUs28Z9vVnH9yW3o06pupOOIiIiISEUy7x3AodtFkU5SOmfcHywn8dEf9t+3cFxoUpqB5Z+rAlPhVw3s2JPL78bMoW1qTW4/o0Ok44iIiIhIRZP2NjTuCfXaRjpJ6SQ1ggG/hyUfw6KP9m0vKICFE6D96RCbELl8FZAKv2rgL+Pms2HHHh67tCcJsernLCIiIiJFbFkK674PunlWJn1vDMYkfvQ7yN0TbFs7C3ZuUDfPA1DhV8VNWbiRN2eu4ZcD2tKzee1IxxERERGRiibt7eB71wsim+NwRcfCWX+FbSvgqyeDbQvHQVQMtB8c0WgVkQq/Kixzdy6/f3sunRol8atB7SMdR0REREQqGneYOwZa9IOUZpFOc/jaDoQu58HUx2D76qDwa3Uy1FCDR3Eq/KqwP30wj627cnj0kh7Ex6iLp4iIiIgUs3EebF4E3SvJpC4HMvjB4PtbI2BLOnRWN88DUeFXRX08bwPvfr+WW05rR7emKZGOIyIiIiIVUdoYsGjocn6kkxy52s3h5DuC8X0AHYdGNk8FpcKvCtq6K4e7351L1ybJ3DywXaTjiIiIiEhF5B6M72szAGrWj3Sao9PvVqjTGpqfAMlNIp2mQtIq3lXQH99PIzM7l9eu60tstGp7ERERETmANTNg+yoYcIC18Cqb2AS4dmLFX3w+glT4VTHj5qxj/Jz13DWkI50aJUc6joiIiIhUVGlvB4ugV5WlD2qlRjpBhabmoCpkU9Ze/vheGj2a1+bGU9pEOo6IiIiIVFQF+TDvXegwGBLUWFAdqPCrItyd/3l3Lrty8nnskmOIURdPERERETmYFdNg50boVoln85TDouqginjvh7VMnL+ROwd3oF2DpEjHEREREZGKLO1tiKsFHc6MdBIpJyr8qoC0tZnc824avVvW4dr+6uIpIlJZmdkoM8sws7SD7Dcze8LM0s1sjpkdW94ZRaQKyMuB+WOh09kQWyPSaaScqPCr5FZt2c3IF7+ldmIcz1x+LNFRmslIRKQSewk41Ov3s4D2oa8bgGfLIZOIVDVLJ8Oe7dDt4kgnkXIU1sLPzM40s0WhN5O/P8D+Fmb2qZl9H3pzOTS0vZWZZZvZD6Gv58KZs7LasnMvI178lrwC5+VrjqdhckKkI4mIyFFw96nA1kMcch7wige+BmqbWePySSciVcbcMVCjTrB+n1QbYVvOwcyigaeBM4A1wAwzG+vu84scdg/wprs/a2ZdgAlAq9C+pe7eM1z5KrvdOXlc8/JM1m3P5vXr+9KuQa1IRxIRkfBrCqwu8nlNaNv64gea2Q0ErYK0aNGiXMKJVAk5u2H9bKjTCpKr4HuVnF2waAIccynExEU6jZSjcK7jdzyQ7u7LAMzsDYI3lUULPwcK549NAdaFMU+VkZdfwK2vf8/cNdt59ore9G5ZN9KRRESkgnH354HnAfr06eMRjiNSMbnD9pWwegas+RZWfwsb5oLnQ/MT4NqPI52w7C3+CHJ3q5tnNRTOwu9AbyX7FjvmT8AnZnYrUBM4vci+1mb2PbADuMfdpxW/QXV8m+nu3PNeGpMXZvCX87sxpGujSEcSEZHysxZoXuRzs9A2ESmN3GxY98O+Im/NjGBJA4DYRGjaG/rfBjsz4PtXYXM61G8X2cxlbe7bkNQYWvaLdBIpZ+Es/EpjGPCSuz9mZicCr5pZN4IuKy3cfYuZ9QbeM7Ou7r6j6MnV8W3mPyct4Y0Zq7n1tHZccULLSMcREZHyNRa4JdSLpi+Q6e77dfMUkZDMNbD6m30teuvnQEFusK9O62CMW7PjoPnx0KArRIf+aZy1AX54HWa/DoPujVT6spe9HdInwnHXQVR0pNNIOQtn4Veat5LXEpq9zN2/MrMEoL67ZwB7Q9tnmdlSoAMwM4x5K7zR367i8clLuKR3M24/o0Ok44iISBkzs9HAAKC+ma0B7gNiAdz9OYKx8EOBdGA3cHVkkopUcIs+gnG/gazQKKKYGtD0WDjxZmjeNyj2aqUe/PykRtDudPhhNAy8u+oUSQvHQX6OunlWU+Es/GYA7c2sNUHBdxkwvNgxq4BBwEtm1hlIADaZWSqw1d3zzawNwbTVy8KYtcKbNH8jd787lwEdU/nfC7tjpmUbRESqGncfVsJ+B24upzgilVN+Lnx4F8QlwlmPQPPjoGE3iI49vOv0HA5vjYBln0G7QWGJWu7mjglaOptqCdDqKGyFn7vnmdktwMdANDDK3eeZ2QPATHcfC9wB/NvMfkMw0ctId3czOwV4wMxygQLgF+5+qOmtq7TvVm3jltHf0a1pCk8PP5bYaC2/KCIiInJAc9+C7atg2H+h46GWxSxBx7MgoXbQ5bMqFH47M2D559D/dlADQrUU1jF+7j6BoFtK0W33Fvl5PnDSAc57G3g7nNkqi6WbdnLtSzNomJzAqJHHUTM+0sMyRURERCqognyY9hg06g4dhhzdtWLiofslwSQv2duhRu2yyRgp894DL4BuF0U6iUSImo4qsIysPYwY9S1RZrxyzfHUrxUf6UgiIiIiFde8d2FLOpxyV9m0avW6HPL2wLx3jv5akZb2NjToAg27RDqJRIgKvwoqa08uV784g627cnjx6uNoWa9mpCOJiIiIVFwFBTD1UUjtBJ3OKZtrNu4ZFEs/vF4214uU7ath9ddq7avmVPhVQDl5Bfzyte9YuCGLpy8/lmOaVfKuBSIiIiLhtmg8bFoAJ98JUWX0T1yzYJKXNTNg0+KyuWYkpIVGUKnwq9ZK/L/CzM4xMxWI5aSgwPntmNl8kb6Zhy/szsCODSIdSURERKRic4fP/wZ120DXC8r22t0vBYsO1vQrD+t+gNcugo3zy+6aaWOCxenrti67a0qlU5qC7ufAEjP7m5l1Cneg6u6vHy/kvR/WcefgDlzSp3nJJ4iIiIhUd0smwoY5wYyV0WU8EV5SQ2h/Bsx+I5g8Jtw+vhvSJ8GoIcFSEkdr02LYMFdr90nJhZ+7XwH0ApYSrLf3lZndYGZJYU9Xzbz45XL+9fkyrjihBTcPbBfpOCIiIiIVnztM/RukNIcel4XnHj0vh6z1sPTT8Fy/0PJpsPIL6PcrSG4atPwd7fjCtLcBK/uWUKl0StWF0913AGOAN4DGwAXAd2Z2axizVSuT5m/kgXHzGdylIfef200LtIuIiIiUxvLPgzF4/W87/EXaS6vDmVCjLvzwn/Bcv9BnD0OtRjDwf+Daj6HlSfDeL+HTh4IC93C5B908W/WH5MZln1cqldKM8TvXzN4FPgNigePd/SygB8EC7HKU8vIL+PP4+XRsmMQTw3oRHaWiT0RERKRUpj4KSY2h5xXhu0dMXLCm38LxkL0tPPdYPjVo7ev/G4itAQkpcPmYoLXx84eDAjAv5/CuuX52sLyFJnURStfidxHwD3fv7u6PuHsGgLvvBq4Na7pq4v0f1rFyy25uP6MDCbHRkY4jIiIiUjms/ApWTAu6RsYmhPdePYdD/t59M2SWJfd9rX29R+zbHhMH5z0NA++G2aPhPxcFi8mXVtrbEBUDXc4r+8xS6ZSm8PsT8G3hBzOrYWatANx9clhSVSN5+QU8OWUJXRonc0aXhpGOIyIiIlJ5TH0EEutD75Hhv1fjHtCwW3jW9Fs+FVZ+CSffHrT2FWUGp/4WLvhXUOiOGgLbV5V8zYICSHsH2g6CxLpln1kqndIUfm8BBUU+54e2SRkYO3sdK7bs5tent9e4PhEREZHSWjMLlk6GfrdAXGL471e4pt/aWZCxsOyuW9jal9QYjh1x8ON6XAZXvA071sMLp8O67w993dXfwI416uYpPypN4Rfj7j92KA79HBe+SNVHXn4BT01Jp3PjZAartU9ERESk9KY9Cgm14bjryu+e3S8Nuk6W5Zp+y6fCqunBUhQldVdtcypc+wlEx8OLQ2HRRwc/Nm0MxCRAp6Fll1UqtdIUfpvM7NzCD2Z2HrA5fJGqjw/mrGPZ5l38elA7tfaJiIiIlNaGubBoApxwE8SX4wpjtVKh/WCY/V/Izzv667nDZw+FWvuuKt05DTrBdZOgfgd4Yxh8++/9j8nPg3nvBbORluefj1RopSn8fgH8j5mtMrPVwO+AG8Mbq+rLL3CenJxOp0ZJDO7SKNJxRERERCqPqY9CXBL0vaH8793zcti5AZZOOfprLf8cVn1Vuta+opIawtUToP0QmHAnfHJPMKav6HV3b4buWrRd9inNAu5L3f0EoAvQ2d37uXt6+KNVbR/MLmzta0+Ulm8QERERKZ1Ni2D++0HRV6NO+d+//WBIrHf0a/q5B+vzJTUpfWtfUXE14bL/wPE3wPQnYcxIyM0O9qW9DfHJ0O6Mo8soVUpMaQ4ys7OBrkBCYZdEd38gjLmqtPwC54kpS+jYMIkhXdXaJyJSFZlZTSDb3QvMrAPQCfjQ3XMjHE2kcpv2WDDz5Qk3Reb+MXHBWL+Z/we7tx75jJnLPoPVX8PQR498KYqoaDjrb1CnFXx8dzDxyyUvwYIPoNPPwr/EhVQqpVnA/Tng58CtgAGXAC3DnKtKGzdnHcs27eLXp6u1T0SkCptK8MK0KfAJcCXwUkQTiVR2W5fB3LegzzVQs37kcvQcDvk5R76m348zeR5ha19RZnDizXDpy7BhDjxzIuzdAd01m6f8VGnG+PVz96uAbe5+P3Ai0CG8saqu/ALniclBa9+Zau0TEanKzN13AxcCz7j7JQS9Z0TkSH3xD4iKhX63RjZH42OgYfcj7+657NOgte/k2yEmvmwydTkPRoyD6Bio2QBaDyib60qVUZrCb0/o+24zawLkAo3DF6lqGz93PUs37eLWQe3U2iciUrWZmZ0IXA6MD22LjmAekcpt+2r4YXTQQpZUAV6e97o8WEtv4/zDO6+wtS+56dG39hXX/Di46Wu49uOgABQpojSF3wdmVht4BPgOWAGU4eIl1Ucwk+cS2jeoxdBuqp1FRKq424A/AO+6+zwzawN8GuFMIqW3fg4s+zzSKfb58vHg+0m/jmyOQt0vObI1/ZZOCRZXL8vWvqJqNYC6bcr+ulLpHbLwM7MoYLK7b3f3twnG9nVy93vLJV0VM2HuepZk7ORXmslTRKTKc/fP3f1cd/9r6Hm62d1/VdJ5ZnammS0ys3Qz+/0B9rcws0/N7Hszm2NmWp1Zyp47vDUSXj0flkyKdBrI2gDfvQI9h0Ht5pFOE6hZP1gn73DW9Puxta8Z9LoyvPlEijlk4efuBcDTRT7vdffMsKeqggpCY/vaN6jF0O5q7RMRqerM7HUzSw7N7pkGzDezu0o4J5rguXsWwTJKw8ysS7HD7gHedPdewGXAM2WfXqq9ldNh61KIqwVjroaMBZHNM/1JKMiD/r+JbI7ieg6HXRmQXsrieOkUWPNt+Fr7RA6hNF09J5vZRVa4joMckQlpQWvfrYPaE63WPhGR6qCLu+8Azgc+BFoTzOx5KMcD6e6+zN1zgDeA84od40By6OcUYF3ZRRYJ+f7VYB2466cESye8fins3BSZLLs2w8xRQdfKitaFsf1gSKxfukle3OGzh0KtfVeEP5tIMaUp/G4E3gL2mtkOM8sysx1hzlWlFLb2tU2tydlq7RMRqS5izSyWoPAbG1q/z0s4pymwusjnNaFtRf0JuMLM1gATCJZbEik72dth3nvQ/WKo3x6GjQ6KvjeGQ+6eks8va189HSxMfvId5X/vkkTHwjE/h0UfBmv6HcrSybBmBpxyh1r7JCJKLPzcPcndo9w9zt2TQ5+TSzoPjm6cgpn9IXTeIjMbcni/VsXy0bwNLN4YjO1Ta5+ISLXxL4IJ0WoCU82sJVAWL06HAS+5ezNgKPBqaAzhT5jZDWY208xmbtoUoZYaqZzSxkBe9r4xaE17wwXPBV0Ux94StFyVl+xt8O2/oev5kFpBVxPrORwKcmHumIMf4w6fPgQpzaGnWvskMkqzgPspB/oqxXlHPE4hdNxlBOsdnQk8E7pepVNQ4Dw+KWjt+9kxTSIdR0REyom7P+HuTd19qAdWAgNLOG0tUHTmimahbUVdC7wZusdXQAKw30rW7v68u/dx9z6pqalH/HtINfTdK8EadU167dvW9Xw47Y/B4ulTHym/LN/8C3Ky4OQ7y++eh6tRN2h0DPzw2sGPSZ8Ma2cGrZYxceWXTaSI0nT1vKvI1x+BDwi6mZTkaMYpnAe8EZpMZjmQHrpepfPxvA0s2pjFraeptU9EpDoxsxQz+3thq5uZPUbQ+ncoM4D2ZtbazOIIXoKOLXbMKmBQ6B6dCQo/NelJ2Vg/O/g69iooPr3DyXdAj2Hw6YOQ9nb4s+zZAV8/Cx2HBsVVRdbz8uDPbUPa/vsKx/altAiOE4mQ0nT1PKfI1xlAN2BbKa59NOMUSnNuhVdQ4Dw+eQlt6tfknB4ltPatmRn0DxcRkapiFJAFXBr62gG8eKgT3D0PuAX4GFhA0Ctmnpk9YGbnhg67A7jezGYDo4GR7uXZ906qtO9eheh4OOaS/feZwTmPQ4t+8O4vYfWM8GaZ8QLs2Q6nVODWvkLdL4GoWJg9ev996ZOC1r5T1NonkVWaFr/i1gCdy+j+pRqncDAVffzCJ/M3sHBDFrcOanfw1r6ty+DNEfDCIBg9DDbOK9+QIiISLm3d/b5Qz5dl7n4/UOKUhO4+wd07uHtbd38wtO1edx8b+nm+u5/k7j3cvae7fxLm30Oqi9xsmPMmdDkXatQ58DEx8fDz1yC5MbwxDLavCk+W3VuDSV3aDgrGGFZ0NetBxzNhzn8hP3ff9qKtfT2GRy6fCKUb4/ekmT0R+noKmAZ8V4prH804hdKcW6HHLwStfem0rl+Tcw40tm/3Vvjof+Cp42HJJ0Hf9YRkmPSncs8qIiJhkW1m/Qs/mNlJQHYE84gc2vyxsDcz6OZ5KDXrwfC3IC8HXv950CWzrBQUwPevwVN9goldBvyh7K4dbj0vh12bYMnEfduWTIS1s4JWS7X2SYSVpnVtJjAr9PUV8Dt3L810REczTmEscJmZxZtZa6A98G0p7llhfDJ/IwvW7+DW09oRE13kjzlvL0x/Cp7oCd88Cz2Hwa++h0F/DPrOL/kElk+LXHARESkrvwCeNrMVZrYCeIpgiSSRiun7V6FOa2jZv+RjUzvApS/DpkXw9rWQn3f0998wF148E96/Geq1gxs/h+bHHf11y0u706Fm6r41/Qpb+2q3CMZGikRYTCmOGQPscfd8CGbrNLNEd999qJPcPc/MCscpRAOjCscpADNDXVbuAP5tZr8hmOilcJzCPDN7E5gP5AE3F96/MnAP1u1rVS+RcwvH9rnDvHeDFr3tK4O/HM54ABp23Xfi8TcEs1dNug+um7z/oGoREak03H020MPMkkOfd5jZbcCcyCYTOYAtS2HFNBh0L0SVctRN24Fw9qMw7jfwyT1w1sNHdu89O4IC6Zt/QY3acN7TQbfI0uaoKArX9PvmuWDR+bXfwbrv4Jwn1NonFUJpCr/JwOnAztDnGsAnQL+STnT3CQSTthTddm+Rn+cDJx3k3AeBB0uRr8KZOH8j89fv4NFLegStfSu/Cv5CXDsTGnaDK96BdoP2PzG2Bgy8G96/Cea/H0ydLCIilZq7F+0Hdzvwz0hlETmo718Fizr8cWh9roHN6fD101C/HRx3XenPdQ9mB/34bti5EXqPDArPxLqHl6Ei6TkcvnoqWPZizn+D1r6eGtsnFUNpCr8Edy8s+nD3nWaWGMZMlZp7MJNny3qJnN88G/57BSz4AJIah95gDYOoQyxJ2OOy4C+MyQ9Ap7ODt0dVmTvMHAVeAMdfH+k0IiLhpq4cUvHk58EPr0P7IcGkLYdr8J9h61KY8Nugq+iBXm4Xt2kxTLgDlk+Fxj1h2OuVYxKXkjTsGvw+nz4UjJc898mq/285qTRK04a+y8yOLfxgZr3R4PSDmrQgg3Xr1vBC6lvEPHcCpE+BgffArbOg1xWHLvog2H/6n4K/QGe9VA6JI2jXlmBQ+PjbYcJdwZIWIiJVm5ZdkIpnySdBi1tJk7ocTFQ0XPQCNOgMb42EjIUHPzZnF0y6H57tF6x7d/ZjcP2UqlH0Fep5eVD01W6psX1SoZSm8LsNeMvMppnZF8B/CdYYkmI8N5tVHzzE1ITbabdyNPS6Mpi45dS7IK6kNXuLaD8YWp4En/8V9u4s+fjKaOV0eK4/LPsUBv8FajWE8XdAQaUZyikickBmlmVmOw7wlQWUsKirSAR890rwHG4/+MivEZ8Ew96AmAR4/dJgjFtR7rBgHDzdF774e7Du3S2zgq6hJb0Ur2y6XwwpzeH0+9TaJxVKiV093X2GmXUCOoY2LXL33EOdUy1tWcqeUedybfYa1jU4haSLH4EGnY7sWmbBxC8vDAq6fQ74fdlmjaSCfJj2WDCIu04ruG4SNO4RdIV9+9qglfO4ayOdUkTkiLl7UqQziJTajvWw5GM46dcQXZoRQIdQu3lQ/L00FN64HEaMDdb927ocPvxdcJ8GXeDqD6FliVNFVF6JdeE3aZFOIbKf0qzjdzNQ093T3D0NqGVmN4U/WiXijo+/nfzd27gt/n5Sb3z/yIu+Qs36QOdzYfqTsDOjbHJGWtYGePV8+PRB6HYx3Dg1KPoAul0ErU4OxjYWf0soIiIi4TH79WCcfa8ry+Z6zXrD+c/C6q/h/Vvg87/BMyfAyi9h8IPBs78qF30iFVhpunpe7+7bCz+4+zZAs3AUNf89bNln/C3nYk4YdCGx0WU0/fCg+yA3O/hLs7JLnwTPnhSM4zvvabjw+aBbSCEzGPoI5OzUIvYiIiLloaAAvns1ePFar23ZXbfbhf/P3n3HR1WlDRz/nfRKSEhCC5CAdEINIKAUQUFEFEUBUcEC4uqqWPZ1XQu21VV2dXVFRREsFBUUQUCUJijSe28JvSSB9DrJef84kxCQkgwzc1Oe72ezSe7M3HlyGefMc+85z2PqG2z9xpzsbdofHl0LXR+VqY9CWKg01/Q9lVLK3l8PpZQnIM1IiuRmwE/PcTq4KVMT+7CyWaTz9h1+lSltvH4yXP2wc9+U3aUgH5a8Br+/a6Z3DJ588auhkc3N37nyfWg/omI1bRVVR/YZ+OnvZj3M9S9bHY0QQjju4G9wJh56/t35++7+NPiFmEbvDXs6f/9CiDIrzaWpn4CvlVK9lVK9genAAteGVYEsfxvSj/F9nSfx9fEhMtjXufvv8X/g6QtLXnXuft0h5RBM7m+Svg4jTdWuy02B7fF/Zr3fvCel0Isofw6vgY+uhc3Tzev62EarIxJCCMdt+BJ8Q6DFQOfvWynoPFqSPiHKkdIkfv8HLAHG2L+2Ypq4i8TdpvhK27v5LachDWoEopSTWzQF1zRTI7Z/D0fXO3ffrrRzrqnambjLXOW7+b+mQf3l+AZD39fhxBbT30+I8qCwEFb8Bz7rZxoc3zMbAmrALy+ZSnVCCFHRZJ+BHT9A6ztLNz4LISq8yyZ+WutCYDWQAHQCrgN2ujasCkBr03vOJxD6jCMhOYuYcBf1te/6VwgIrxgfMvNzYN7TpnF9WEOziLvVbWXbR8vbIKa7ucqZkeiaOIUorYxTMPV2WPwyNL8ZxqyARr2g+98g/lfYv8TqCIUQouy2fAsFuY737hNCVDgXTfyUUk2UUi8ppXYB7wOHALTWvbTW/3NXgOXW9u/Nh77rXsDmX4PDp7OIrlGGXn1l4RtspkAmrDBFUsqrpH0wqQ+s/QSufgTu/xnCYsq+H6Wg/3h7k9dxTg9TiFI7sMxcuT64Ega8A3dMMWtWAOLuM815F71krggKIURFobXp3Ve7DdRubXU0Qgg3udQVv12Yq3sDtNbXaK3fB2TRFUBuOix8Dmq1hrj7OXImG1uhJjrcRYkfmDVyoTHmql95XPu2+Wv4uDukHoFhX0O/f4LXFdQAimgKXR6BTV/BodXOi/NCCmyu3b+ogCflwQAAIABJREFUeApspijRF7eaRG/UEoi735yUKOLlC71fhBNbYdtM62IVQoiyOr4JTm6Vq31CVDGXSvxuA44DS5VSn9gLuzh5AVsF9etbkH4cbvoPeHgSn5wJQIwrEz8vH+j9ApzaDlu+cd3zlEVmEqz6yCR83482Zw7H/A5N+zln/93/BsF1YP5TrkvO1k6C12vB5Jtg41cmqRdVW+pR+PxmU7ip7XAYvQxqtrzwfVveZl73S14FW647oxRCCMdt+AK8/E1PXSFElXHRxE9rPVtrPRRoBiwFngAilVIfKqVucFeA5c6pXbBqgml0am83kJBkEj+XTfUs0mIQ1G5reuLk57j2uS7GlmsWg08fBv9uCj/9n5ky0n88jJgLIXWd91y+QebK4Ymtzi/0orX5YD/vSajb3iTyPzwCbzeGWaPMuq3yeGVVuNbuBfBRNzi+GQZNhFs/MOt4L8bDA/q8bCrYrp3kvjiFcLMVexN5dNoGCgvL+TpzcXl5mbB1JrS8FfyrWx2NEMKNLtvHT2udCUwDpimlQoE7MJU+f3ZxbOWP1jD/afAJgj7jijcnJGUS5OtFeJCL2xt6eMD1r8AXA806uq5/de3zFdHaVBTdNA22zYKcFNPD7OqHoc2wi18NcYYWt5pS0EteM4NUkBP6JBYWws/Pw6oPoPUQ01DewwuOrDVl+rfNMk1ng+uYamdthl2+DYWo2Gx5Zq3eqglQKxYGTzF9NEujUS9o2MucSGg3/OwaQCEqkaSMXH7ccpzujSO4s2M9q8MRV2LHD5CbZk5gCyGqlNK0cyimtT6jtZ6ote7tqoDKtW2zTIGV3i9CYHjx5vjkLKLDA5zfyuFCGvaAq/rA8vGmFLMrpR4xz/O/jvBpb9g01Tz38Fkwdgfc8Jprkz4wa6pufBvys8z6xitVYDNX9lZ9AJ3HwK0fgae3eZ56nUwBj6f2mCIetVubZvITOsPEnrD6Y8hMvvIYRPly+gB8doNJ+jqNhgcWlT7pK3L9y5B9Gn571zUxCmGxW9vWpX396ry1cBdpOflWhyOuxIYvIawRNOhqdSRCCDcrU+JXpeWmw8J/mKmWHUaec1NCUqbrp3mW1Gcc5KS65kNmbgZsmm7WOL3TyqxdCoyAm9+Dp/fA4EnQuA94XvZisfNENDG9DDdPg0OrHN9PfjZ8c4/ZT69/QL83zVXU83n7QctBcNfX8NQu6PsGFNpgwd/g301g+l2mT6Etz/FYRPmwbRZ81N0kf0O+gv5vm3//sqrdBmLvgFUfQtox58cphMWUUrw8sBXJmXm8t2iv1eEIRyXthUMrTVEXd5ysFkKUK5L4ldayNyHjZHFBlyJ5tkKOnMlybWGX89WKNVMUV39krspdqcJCOPArfD8GxjeB2WPMmqWez8Jjm+D+BdBhhLVT2Lo/A9WiTI9ARwq95KTCV4PNGq7+46HH30o36AVFQpe/wJjfTOGazmPMlNCv7zZJ4Lyn4cS2sscjrJWfDXMeg5n3m2m8Y34zPfquxHXPgy6AZW84J0YhypnYqBCGxNVjysoE9p2SQlgV0oYvzNKGNsOsjkQIYQFJ/Erj5A5zJr/9vRDV4ZybDp/JolC7obDL+Xo9B7oQll7Bh8ykvbD4FXg31qwb3DUPYm+H+34yCV/PZx3rw+cKPoGm0MvJrbCujEU0MhJhygA4vApu/xQ6jXIshlqtoO/r8OROGD7TrOva8IWZBipNvCuO7DPwxS2w4XPo9gTctwCq17/y/YZGQ8cHTXXYxN1Xvj8hyqGn+zbF38eTl+fuQGsp9FKhFOSbdexN+kFwTaujEUJYQBK/y9Ea5j8DftWg95/XmBVX9HTnFT+A0AZmPdLmaSYxLa2s07D2U/ikN/wvDn57x1zxuH2Smco58H1o0KV8TgFpPhAaXWcKvWScKt1jUg7BZ31Nkjvsa4h1QulqTy9ofD3cMdkkgRFN4et74NjGK9+3cK30E6Z1x9ENZh3n9S+bNZ7Ocq29+NOil523T1HlKKX6KaV2K6X2KaWevch97lRK7VBKbVdKTXNXbOFBvozt04QVe5P4ZcdJdz2tcIY9P0FmovTuE6IKk8TvcrbOhIO/maQvsMafbo5PckMPv4u59inwCYbFl/mQWZBvpjh+fY9pwTDvKVMs5fpXTeJy9yyTEHn7uyduRxUXesmGX168/P1P7YJJfSErCe79waxNdLbAGubqn3+YmUqavN/5zyGc43S8OQlwJgGGf2PWcTpbYA3o9jjsnndl61ErK61h3WTIy7I6knJLKeUJfADcCLQAhimlWpx3n8bA34FuWuuWmHZLbnNPlwY0jgzitXk7ycmXtjcVxoYvTLXqRlWzPp8QQhK/S8tJg5//AXXaX/QMWUJyJtX8vAgNcOJVg9IKCINrnjBn8RJ+P/c2reHYJljwLPy7GUwfCgdXQtwD8NByeHgldHsMgmu5P+4rEX6ViXvzdPP3XMyR9TC5n1lzNXI+1O/supiq1YZ7vjNTb7+6DdLlLHi5c2KbSfpyUmHEHHPl2FWufhiCapmTEzIV7qzCAvhxLPz4hKkQLC6mE7BPa31Aa50HzABuOe8+o4APtNZnALTWpZwC4Rzenh68dHNLDp3OYtJv8e58auGo1KOwb5FpOePO4mxCiHJFEr9LWfammVJ40/hzCrqUlJBkCru4pZXDhXQeY87gLXrJfMhMPwG/vwcfdoWJPcx6uAZdYdgMU6HyxjdNBcLyOJWztK59CkLqXbzQy/6lpiqpXwjcv9CszXO18MYw/Fvzepk62Jw0EOXDoVUwpT8oT7N+NSrOtc/nEwi9/g6HV5t1swJsuaaQzvrJcM1YsxZSXExd4HCJ34/Yt5XUBGiilPpdKbVKKdXPbdHZXdM4nL4ta/K/Jfs4nprt7qcXZbVpmjk52Xa41ZEIISwkid/FnNxuqmZ2GAF1O1z0bvFJme5f31eST4D5kHlkrem195/m8MsL5sPnTf+Gp3bDkC+h6Y3OXctkJZ9A6PcGnNpuGtmXtH02TL3DFNq4f6F7i9NExcGdX8CpHabqpy3Xfc8tLmzvL/DFrRAQDg8sNOtZ3aHt3RDexEzDdqQKbWWSmwHThsCO2ab3Z59xFfvEU/ngBTQGegLDgE+UUtXPv5NSarRSap1Sal1iYqLTg3j+phYUaM0b83c5fd/CiQoLYeMXENOj/BRsE0JYwqWJ3+UWqCul3lFKbbJ/7VFKpZS4raDEbXNcGeefaG2uJl2koEuRnPwCjqVmu7+i5/na3GX6C6afNGfTH10HDy4yZ9UDwqyNzVWaDTDN5Jf+01zlBFg/Bb4daRL1++ZZM4218fUw8H8Qb2+PUVjo/hiEsXWmmeIc3ticBHBG5c7S8vQy7x1Je2DTV+573vIm67SpoBr/K9zyAXT9q9URVQRHgXolfo+ybyvpCDBHa52vtY4H9mASwXNorSdqreO01nERERFXFpUtD46sO2dTvbAAxnRvyJzNx1ibcPrK9i9cJ/5XU+hMiroIUeW5LPErzQJ1rfVYrXVbrXVb4H3guxI3ZxfdprUe6Ko4L2jLN6bBaZ9xl0ycDp/OQmuLCruU5OkFo5fB2G3Q+0XzQbeyUwpufAtsOfDzC7DiPzD3cZMM3vM9+IdaF1vbYdDnZdj+HSx8TtZ5WWHNJzDrQajXGUb+CEFX+KHXEc1uMs+/9I2qWcwk7RhMvhFObIU7v4R2d1sdUUWxFmislIpRSvkAQ4HzT37OxlztQykVjpn6ecClUf36L/isH+z44ZzNY3o2onaIHy/9sJ2CQnmvK5c2fmnGxGYDrI5ECGExV17xK80C9ZKGAdNdGE/p5KTCz8+bq0btLn12LN6qVg4XolTVmz5VoxF0fQy2fmOm1MXeAcOmm+mvVuv2OFz9F1j9Ifz+rtXRVB1aw7J/wfynzfTmu2eZtZ5WUAqufwUyTsCqCdbEYJXk/aaibupRuHsmNJcPnKWltbYBjwILgZ3AN1rr7UqpV5RSRSdBFwLJSqkdwFLgGa11sksD6/YY1G1vZlVsOjtUB/h48Vz/5uw4nsaMtYdcGoJwQNZp2DkXWg8Bbz+roxFCWMyVpZ0utED9gqUVlVINgBigZBdsP6XUOsAGvKm1nu2qQM+x9A3T5+aur8Hj0nlxQrK9lYPVUz2rsmufgvjlUK+TaU9xmX8zt1EKbnjdFHtZNA6CakLbu6yOqnIrLISFfzdrc9sMM1Nura5eV/9qaHoT/P5f6HDfBVvCVDrHN8NXt5tCEiPnQp12VkdU4Wit5wPzz9v2YomfNfCk/cs9/ELMbIrpw2D2GMjPLC7SM6B1bb5cdZDxC3czILYOIVZUuRZnaW2m5W6bCdu/h4I8aHeP1VEJIcqBcvIpmaHATK11yYZADbTWccBdwLtKqUbnP8jpC9dPbIM1EyHuPnNm8zLik7IIDfCWQc5KPgHw4C/Q9/Xyk/QV8fCAWz+Ehj3hh0dhz89WR1R5FeTD9w+ZpO/qR+CWCdYnfUV6vwh5GbBivNWRuF7C7zBlAHj6mnWVkvRVLj6BcNc30ORG0w/29/8CoJRi3M0tSc3O551FeywOsgo7uQMWvwLvtYVJfUzPzKiOptesO6pbCyHKPVd+Ui7NAvUiQzlvmqfW+qj9+wFgGfCnTxBOXbiutZke5hcC171QqockWF3RU5R/Xj5mfVPNlvDtiD8VR6iytIZVH5mroVtnQuIe0+fNEXlZMGO4mfJ73Qvl7yRAZDOzvm3NJ6Z5fGW1e4HpYxlcy1RQrQprjasibz9TKbrV7aZX5ZLXQWta1KnG8M4N+HLVQXadkHY2bnMmAVb8GyZ0hQ+7wG/vQFhDc/Lrmb0wdKopOiaEELh2qmfxAnVMwjcUc/XuHEqpZkAo8EeJbaFAltY6175wvRvwlgtjNdM7s1Pg+pdLXQkzITmTLg2rwNQtcWX8qpm1ZpOuN60mHvhZPhQvedV8WFGeUHSh38sfIpubM9O1WkPNViZh9qt28f1kp5jKnYdWwYB3IO5+98RfVj3/Dlu+NR+Sb//k8vevaDZNhx8eMT1Ch8+sGlNaqzJPb7jtE/AOgOVvQV4m9H2dJ69vwpzNx3h5zg6mjepsXX/byi7jlJnCuXUmHFljttXrDDe+DS1vhaBIa+MTQpRbLkv8tNY2pVTRAnVP4LOiBerAOq11UZWyocAM+5qFIs2Bj5VShZirkm9qrXe4KlbAvFGOWWE+iJZCdl4Bx1Nz5IqfKJ2gSLj7O/isL3x5m0n+qtW2OiprrP7YJH0dRprKrEl7TOXHE9vg5FbY+SNs+OLs/UOjTRJYK/bs9+r1zYefr26HxF0w+DNodZtVf9HlVasDVz8Mv/0Huj5qEqTK4o8JZm1lTA9zdcE32OqIhDt4eMLN74FPEKz6APLSCR3wLk/f0IQXftjOgm0n6B9bAd/jtDYtgk5uN+9HJ7bByW2Agk6jzFptb3/3x5WdArt+NMle/K9mDW3NVqZtTKvbIbSB+2MSQlQ4SleSUvNxcXF63Tr3TaPbdSKNfu+u4L1h7RjYpo7bnldUcMc2mjVQ1RvAffPB/089lyu3bd/BzPuhaX/T7P5C6/C0Nq0ATm6zJ4Rbzc/J+wH7+5VviHlsfraZdnZVH7f+GQ7JTjFrb+q0M0UyKjqtYenrsPxtaH4z3D4JvHzd9vRKqfX2deCiFFw2RpZ8HbQajG3gBAZMWE16jo1FT/bA36d0J1MtYcuFxN3m/eXk9rPvNVklCqSG1DMJVsZJOLYBAsKh8xjo+IDr++QW5MPeX2DTVNj7synSEhoNrQZD7GAzQ0IIIc5zqfGxnFQ/qHgSkqSip3BAnXYmUZl6p1mXdvesqlNiO365KcBS/2oYPOnixVeUgpC65qtJ37Pb8zJN8YKis/Bpx0xV13od3RP/lfKvDt2fMb0d9y+FRr2sjshxhQUw/xlYN8lUC7z5v+YKkKh6lILrnjeFXxaNwys/m5dvGs+QSRv5ePl+nujTxOoIjYxEOLHFfkLJnugl7YZCm7ndy88kUk37n51dULPl2ZNzWsPB301Bm6WvmbV0HUaYtj3V6138eR1xahds+go2fw2ZpyAwAuIeMMle3Q5Vr3WTEMJpJPFzUHySacgcHV4OesaJiqXRdTDoI5j1AHw3Cu6YUvk/NB/fAtPvgrBGpteiI1OlfAJNkldREr0L6figvajNS2ZqZHkqQlNatjyTwG//zvSr7POyfBAVcM1YM+1z/tN0zs9kUKu/8eGy/QzuEEVUqIXj5OkDsPhV83otElzHrCVu0td8r9nKvDddqhKwUhB9jfk6uR1Wvm+qgK/+2CRkXR+7ssqZOamwbRZsnApH14GHFzTpZwpDXdXHrKsUQogrJImfgxKSMgkP8iHYT96MhQNiB5upQwufM0Vf6l1tL2oSC+FNTTXQyuJMAkwdbC9yMxP8Q62OyDpevubqyPejTQXMqLiz6xZDY8p/IpiRaE5YxP9qEr5rnrA6IlGedBplTtD88Ahv1s5ihRrDP+fvZMLwDu6PJSPRFJ5Z9xl4+kC3J8xJt1qxVz5Fs2ZLc/Ku1z9g1Yewfgps+dokaN0eh+hrS3cypLAQElbAxq9g5xyw5UBEc9MDtvUQCLrCauVCCHEeSfwcFJ+cSbRM8xRXossjgIKt35opc7Ycs93DGyKalihm0gpqxlbMSomZSaaYjS0X7p8DIVFWR2S92DvMlLN9i2DFf85WNfUJgsgW5t+96N+8ZgvzQbo8OPCruUKdnWJKxbcbbnVEojyyFz/xnfUg80Lept/Wx1m5vwFdG4W75/nzMuGPD8yUzPxsaH+PqaobXMv5z1W9HvT7J/R4BtZOMn1EP78Z6rQ3CWDzmy88m+PMQdg83azdSzlk1iy3HW7+m6rTXq6gCyFcRoq7OKjT64vo3iSC8XdUoup8wjoFNji9/2xxgRP2wiYZJ87ep2h6UtEVolqxpl9TeZ0mmpthPgSd2gH3zoH6na2OqPzJz4HEnSWqmtr/7XNT7XdQUKPR2RMARW0uqtVx34fDAhv8+i9TvCO8MQyeXC6aQUtxl7Jx9xjJnp/R39xDQkEEzwW9xpdPDMTL04VXtAvyYeOXsOxNM5ui2QBT8TLCjWsM83NMQrfyffN+HhoDXf9qkmGAnXPN1b34XwEFDXuaqZzNbrKmUqgQolK61PgoiZ8DMnNttHxpIc/0bcojva5yy3OKKiozqUQyaE8OShYk8A4w04va3lW+1oEU5MO0IXBgKQyZCs36Wx1RxaG1uQpQXITCXtm0ZPP3wEjo9hh0Gu3aSpqpR2HWg3Bopbki0f/tcnMFUhK/snF74gcQvxzbV0M4kh/Muh5TGNy7q/OfQ2uTUC1+GZL3mWnz179i7YmmwgLYNQ9+fxeOrjeVQAvyzQmd6g1MstdmmPOLwgghBFLV0+kSkk1FT5nqKVwuMNxUfyxZAdKWa3rXndhmPlTs+MGsDwmMMNMI2ww1V4asmi5UWAg/PAr7F5s+X5L0lY1SpidXaANzJaBITpq5enpiK+yeDz8/D2s+gT4vQcvbnP/vvfsnmP2web0Nmghthjh3/6Lyi+mO58gfiPhsENesuJvU+lMJaRjnvBNUB/+AX140TczDm8DQaaYqp9VTJT08ocVAM9Xz4EpY87E5Sdd2ODToVv7X8gohKi254ueAeVuO88i0Dcx77Bpa1glxy3MKcVFFvZ42T4c9P5leT5EtTQLY+k7XrG25lJ9fgJXvQa/nzdoX4Rr7l5hjfXIb1I2DG16DBl2ufL+2PHP15I//menEg6dAePmb2SBX/MrGkit+dge3ryLomzuoodLQHt6o8CamdUJkc7OuNbK5uRJW2oTo1C5YNA72LIDg2mYNX9vhl67KKYQQVYRc8XMyueInyhVPb3NVrVl/yDptypZvmg6/vGDaBjTqbZJAd6wj+eMDk/R1HAXdn3btc1V1ja6Dh3qYhH/JazC5n7nC0Odlsy7QEafjYeb9plF1p9Fw/atVp8+kcJkGLa/m06u/YduKOfSNOM0N1c7geWQNbJt59k7eAaaoVVEiWJQUBtc+ewUv7Rgs/acpiuITBNe9YPro+UhbJSGEKA1J/BwQn5RJZLAvgb5y+EQ5ExBmesV1fBAS98CWGaYJ8KwHTOW4lreatSX1r3b+dKgt35r2FC1ugRv/Zf10q6rAw9OsF2o5CP6YYJpK715g/v27/61slWC3fQdzHzf/bnd+aaaqCeEkD97Yhc+Ca/GXeTto61udSaM6EuaVC4m7zRTmUzvN932LTGJXxDfEJIEhdWHXfLO+ufMYuPbpilnpWAghLCRTPR0w+MOVeHgovnnICdOqhHC1wkJIWA6bZ8COOZCfCaHRJgGMvcNUBr3SJG3/Eph6J9TrDHfPkqtEVkk/CcvegA2fg08wdH8KOj106X+P/Gz46e+wfjJEdYTbJ5n1heWcTPUsGyunepa0YOtxHv96E3Wr+/P5fZ2oX+MCV+uyTp9NBE/tNF/J+6BhD9MHMzTa7XELIURFIVU9nf1cr/1C72Y1+dfg1m55PiGcJjfDFILZPB3iVwAa/MPO9o0r6iFXlibyxzbClAHmw9h988FP1r1a7tQuU/Ri70IIqQ+9X4RWt/95DVXibvj2Pji13TS4vu758lMZ9jIk8Sub8pL4AaxLOM2DX6zDy0MxaURH2tSrbnVIQghRacgaPydKz8knKSOP6HBZ3ycqIN8g0/qh7V2QctgUgzlhbxfwpybyzf7cNzAg7Nz9Je+Hrwab5HH4TEn6yovIZjD8GziwzFT//O5BWPUB3PA6RHczJfA3TYX5z5i1VcNnQeM+Vkctqoi46DBmjunKyMlrGDpxFROGt6dXs0irwxJCiEpPEr8ySkjKAiAmXBaTiwquej3oNOrs7xdqIr9/qbk6WKSoiXytWIhoDktfA10I93wH1Wq7/28Ql9awJ4xeDlu+hiWvwpT+0PQmUwxj67cQfS3c9on82wm3uyoyiO/+0pX7p6zlwS/W8fqtrRjaqb7VYQkhRKUmiV8ZxRdV9JQrfqKy8fQyVfUimkLs4LPbi5rIn5MQLjFFFrwDYMRcCG9sXdzi0jw8oO0wU9hn1QRY8Y5Z59nrH3DtU6ZAjBAWiAz2Y8boLjwydQPPfreVY6k5jO3TGCWFoYQQwiUk8SujhCST+DUIk8RPVBGXaiIfEG6q7Ynyz9vfJHrtR0BWsknwhbBYkK8Xn46I47nvtvLe4r0cS8nmjdti8faUJudCCOFskviVUUJSJrVD/PD3kbPkogrz8oXabayOQjgiMNx8CVFOeHt68Nbg1tSp7s9/F+/lZFoOH97dgSBpmSSEEE4lp9TKKD45Uxq3CyGEEE6klGLs9U341+2xrNyfzJCP/+BUWo7VYQkhRKUiiV8ZJSRlyvo+IYQQLqOU6qeU2q2U2qeUevYS97tdKaWVUpWmrcWQjvX5dEQc8UmZDJqwkn2n0q0OSQghKg1J/MogNSufM1n5UtFTCCGESyilPIEPgBuBFsAwpVSLC9wvGHgcWO3eCF2vV9NIvh7dhVxbIbd/+AdrE05bHZIQQlQKkviVQXFFT5nqKYQQwjU6Afu01ge01nnADOCWC9zvVeBfQKWcDxkbFcL3f+lKjSAfhn+6mgVbj1sdkhBCVHiS+JVBUUXPGJnqKYQQwjXqAodL/H7Evq2YUqo9UE9rPc+dgblbvbAAZo3pSmzdEP4ybQOvz9tBana+1WEJIUSFJYlfGcQnZaKUGYyEEEIId1NKeQD/AZ4qxX1HK6XWKaXWJSYmuj44FwgN9GHqg50ZElePT3+Lp+fbS/l8ZQL5BYVWhyaEEBWOJH5lkJCcSZ0Qf/y8pZWDEEIIlzgK1Cvxe5R9W5FgoBWwTCmVAFwNzLlQgRet9UStdZzWOi4iIsKFIbuWn7cnb97emrmPXkOzWtV4ac52+r67nMU7T6K1tjo8IYSoMFya+F2uMplS6h2l1Cb71x6lVEqJ20Yopfbav0a4Ms7SSkjKlGmeQgghXGkt0FgpFaOU8gGGAnOKbtRap2qtw7XW0VrraGAVMFBrvc6acN2nVd0Qpo3qzKf3xoGGBz5fx/BPV7P9WKrVoQkhRIXgssSvNJXJtNZjtdZttdZtgfeB7+yPDQNeAjpjFrq/pJQKdVWspaG1Jj4pk2ip6CmEEMJFtNY24FFgIbAT+EZrvV0p9YpSaqC10VlPKUWfFjVZOLY7Lw9syc7jaQx4/zee+XYzJ6XvnxBCXJKXC/ddXJkMQClVVJlsx0XuPwyT7AH0BX7RWp+2P/YXoB8w3YXxXtKZrHzScmxS0VMIIYRLaa3nA/PP2/biRe7b0x0xlTfenh6M6BrNre3q8sHSfUz+PZ4ftxznoR4NGd29IQE+rvx4I4QQFZMrp3petjJZEaVUAyAGWFLWx7pLvFT0FEIIIcqVEH9vnuvfnEVP9qBXswjeXbSXXuOX8e26wxQWyvo/IYQoqbwUdxkKzNRaF5TlQe6sWFbUyiFaEj8hhBCiXGlQI5AJwzswc0wXaoX488zMLdz8v99YuT/J6tCEEKLccGXid7nKZCUN5dxpnKV6rDsrliUkZ+KhoF6orPETQgghyqO46DC+f7gr/x3alpSsfO76ZDUPfr6O/YkZVocmhBCWc2Xid8nKZEWUUs2AUOCPEpsXAjcopULtRV1usG+zTHxSJlGhAfh4lZeLpEIIIYQ4n4eH4pa2dVn8VA+e6duUVQeS6fvOcj77LV7aPwghqjSXZTFlqEw2FJihS7wb24u6vIpJHtcCrxQVerFKQnKmTPMUQgghKgg/b08e6XUVS5/uSc+mkbzy4w4em7GJzFyb1aEJIYQlXFr2qjSVybTW4y7y2M+Az1wWXBlorUlIyqJDfUs7SgghhBCijCKCfZl4Twc+/HU///55N7tPpPHR3R1oGBFkdWhCCOFWMm+xFJIy8sjItckVPyGEEKIC8vBQPNLrKr64vzNJGXkM/N/v/LTIy25tAAAgAElEQVTtuNVhCSGEW0niVwoJyVLRUwghhKjormkczo9/vYZGkUGM+WoDbyzYia2g0OqwhBDCLSTxK4XiHn7SvF0IIYSo0OpU9+ebh65meOf6fPzrAe6ZtIbE9FyrwxJCCJeTxK8UEpIy8fJQRIX6Wx2KEEIIIa6Qr5cnrw+K5d93tGHDoTPc/P5vrD94xuqwhBDCpSTxK4WE5EzqhQXg5SmHSwghhKgsbu8QxXd/6YqPlwdDJ/7B5ysTpOWDEKLSkkymFOKTsoiuIY3bhRBCiMqmZZ0Q5j56Dd0bR/DSnO2M/XoTWXnS8kEIUflI4ncZWmsOSg8/IYQQotIKCfDmk3vjeOr6Jvyw+RiDPlhZvL5fCCEqC0n8LuNUei5ZeQXESOInhBBCVFoeHoq/9m7M5/d14lR6DgPf/42ft5+wOiwhhHAaSfwuo+iMX7RU9BRCCCEqve5NIpj712uIiQhk9JfreeunXRQUyro/IUTFJ4nfZSQUtXKQK35CCCFElRAVGsA3D3VhWKf6TFi2n2ETV7H5cIrVYQkhxBWRxO8y4pMz8fH0oE51aeUghBBCVBV+3p68cVss4+9ow77EDG754HdGf7GO3SfSrQ5NCCEcIonfZSQkZVIvzB9PD2V1KEIIIYRws8Edolj+t148eX0T/tifTL//LufxGRuLZwQJIURFIYnfZSQkZck0TyGEEKIKC/L14rHejVnxf70Y06MRP28/Se///Mrfv9vCsZRsq8MTQohSkcTvEgoLNQnJmVLYRQghhBBUD/Dh//o149e/9eSeqxswa/1Rer69jJfnbicxPdfq8IQQ4pIk8buEE2k55NoKpYefEEIIIYpFBvsxbmBLlj7Tk0Ht6vLFHwfp/tZS3vppF6lZ+VaHJ4QQFySJ3yVIRU8hhBBCXEzd6v78a3Brfhnbnetb1GTCsv1c89YS/rdkL5m5NqvDE0KIc0jidwnxyfYefpL4CSGEEOIiGkYE8d6wdix4/Fo6x9Rg/M976P7WUj5dcYCc/AKrwxNCCEASv0tKSMrE18uD2tX8rA5FCCFEFaGU6qeU2q2U2qeUevYCtz+plNqhlNqilFqslGpgRZziz5rXrsanI+L4/i9daV67Gq/N20nPt5fxzi97WLkview8SQKFENbxsjqA8iw+KYsGNQLwkFYOQggh3EAp5Ql8AFwPHAHWKqXmaK13lLjbRiBOa52llHoYeAsY4v5oxcW0qx/KVw925o/9ybyzaA/vLdmL1uDloWhVN4TOMWF0jA4jLjqU6gE+VocrhKgiJPG7hITkTBrKNE8hhBDu0wnYp7U+AKCUmgHcAhQnflrrpSXuvwq4260RilLr0qgGXRp1ITU7nw0Hz7Am4TRr408z+fcEPl5+AICmNYPpGBNKx+gwOsWEUTvE3+KohRCVlSR+F1FQqDmUnEXvZpFWhyKEEKLqqAscLvH7EaDzJe7/ALDApRGJKxbi702vZpH0sn+myMkvYPPhFNYmnGZ1/Gm+33CUr1YdAqBemL9JAqPD6BgTRsPwQJSSmUdCiCsnid9FHEvJJq9AWjkIIYQon5RSdwNxQI+L3D4aGA1Qv359N0YmLsfP25PODWvQuWENHgVsBYXsPJ5efEXw192JfLfhKADhQT50ignj6oY1uLphDRpHBkkiKIRwiCR+F5FQVNFTmrcLIYRwn6NAvRK/R9m3nUMp1Qf4B9BDa33BzuFa64nARIC4uDjt/FCFs3h5ehAbFUJsVAgPXBOD1pr9iZmstSeCqw4kM3/rCQDCAn3oHBNG55gwrm5UgyaRwVKLQAhRKpL4XYT08BNClFV+fj5HjhwhJyfH6lAqDT8/P6KiovD29rY6FHdZCzRWSsVgEr6hwF0l76CUagd8DPTTWp9yf4jC1ZRSXBUZxFWRQQzrVB+tNUfOZPPHgWRWHzCJ4IJtJhEMDfA+54pg05qSCIryScZI53JkfJTE7yLik7Lw9/akZjVfq0MRQlQQR44cITg4mOjoaJmK5QRaa5KTkzly5AgxMTFWh+MWWmubUupRYCHgCXymtd6ulHoFWKe1ngO8DQQB39pfZ4e01gMtC1q4nFKKemEB1AsL4M44c0H48OksVtuvBq6OT2bh9pMAVA/wplN0GJ0b1uDqhmE0r1VNEkFRLsgY6TyOjo8uTfyUUv2A/2IGr0+11m9e4D53AuMADWzWWt9l314AbLXfze2DWkJyJg1qBMgLUwhRajk5OTKgOZFSiho1apCYmGh1KG6ltZ4PzD9v24slfu7j9qBEuVOUCA7uEAXA0ZRsVh9ItieCp/l5h0kEg329aFY7mGa1qtm/B9OkZjDBflXmKrooJ2SMdB5Hx0eXJX6l6UWklGoM/B3oprU+o5QqWUIzW2vd1lXxXU5CUiZNawVb9fRCiApKBjTnkuMpROnUre7Pbe2juK29SQSPpWSzOj6Z9QfPsPtEOrM3HiV9la34/vXC/E0yWOtsUhhdIxBPuTooXEje053HkWPpyit+l+1FBIwCPtBanwEoL2sVbAWFHDqdRd9WtawORQghSi05OZnevXsDcOLECTw9PYmIiABgzZo1+PhcvFH0unXr+OKLL3jvvfcu+Rxdu3Zl5cqVzgtaCOESdar7M6hdFIPamURQa83RlGx2n0hn14l0dh5PY9eJdJbsOkVBoan94+vlQZOa5qpgs9omKWxRuxqhgdJkXlR8Mka6NvErTS+iJgBKqd8x00HHaa1/st/mp5RaB9iAN7XWs89/AleVqj6ako2tUBMjFT2FEBVIjRo12LRpEwDjxo0jKCiIp59+uvh2m82Gl9eF3/bj4uKIi4u77HOU5wFNCHFxSimiQgOICg2gd/Oaxdtz8gvYdyqDXSfS2XU8jd0n01m6O5Fv1x8pvk+DGgG0rVe9+KtFnWr4enla8WcI4TAZI60v7uIFNAZ6YkpWL1dKxWqtU4AGWuujSqmGwBKl1Fat9f6SD3ZVqep4e0VP6eEnhKjoRo4ciZ+fHxs3bqRbt24MHTqUxx9/nJycHPz9/Zk8eTJNmzZl2bJljB8/nh9//JFx48Zx6NAhDhw4wKFDh3jiiSd47LHHAAgKCiIjI4Nly5Yxbtw4wsPD2bZtGx06dOCrr75CKcX8+fN58sknCQwMpFu3bhw4cIAff/zR4iMhhLgQP29PWtUNoVXdkHO2J6bnsvtEOluPprLp8BlWHUjmh03HAPDx9KB5nWq0K5EMSl0EURFVtTHSlYlfaXoRHQFWa63zgXil1B5MIrhWa30UQGt9QCm1DGgH7McNEooTvwB3PJ0QohJ6ee52dhxLc+o+W9Spxks3tyzz444cOcLKlSvx9PQkLS2NFStW4OXlxaJFi3juueeYNWvWnx6za9culi5dSnp6Ok2bNuXhhx/+U8nojRs3sn37durUqUO3bt34/fffiYuL46GHHmL58uXExMQwbNgwh/9eIYR1IoJ9iQj25ZrG4cXbjqdms+lQCpsOp7DxcApfrz3MlJUJgGkr0caeBLapV522UdVliqi4KBkjrRkjXZn4XbYXETAbGAZMVkqFY6Z+HlBKhQJZWutc+/ZuwFsujPUcCclZBPp4EhEkrRyEEBXfHXfcgaenmZaVmprKiBEj2Lt3L0op8vPzL/iYm266CV9fX3x9fYmMjOTkyZNERUWdc59OnToVb2vbti0JCQkEBQXRsGHD4vLSw4YNY+LEiS7864QQ7lI7xJ/asf7cGFsbMDUR9pzMYNPhFDYdPsOmwyn8uicRbZ+DFV0jgNoh/gT6ehHo62m++5jvQb5eBPjYt/t4nXMfc5vZLq0ohKtVpTHSZYlfKXsRLQRuUErtAAqAZ7TWyUqprsDHSqlCwAOzxm/HRZ7K6eKTMokOD5QpC0IIhzly1tFVAgPPTlt/4YUX6NWrF99//z0JCQn07Nnzgo/x9T174svT0xObzebQfYQQlZeXpwct6lSjRZ1q3NXZ1FrIyLWx5Yi5KrjlcCrJmbkcTckmK89GZq6NjFwbOfmFpdq/j6cHN7SsyV2d69OlYQ35XFaJyBhpDZeu8StFLyINPGn/KnmflUCsK2O7lITkzD/NdRdCiMogNTWVunXrAjBlyhSn779p06YcOHCAhIQEoqOj+frrr53+HEKI8ivI14uujcLp2ij8ovcpKNRk5tnIyi0gI9dGVp5JCDNzC4p/zsot4PCZLH7YdIwftxynYXggd3Wuz+3to2QKqXCZyj5GWl3cpdzJLyjkyJlsbm5dx+pQhBDC6f72t78xYsQIXnvtNW666San79/f358JEybQr18/AgMD6dixo9OfQwhRsXl6KKr5eVOtFE3kn+vfnPlbjzN19SFem7eTtxbupn+rWgy/ugFxDULlKqBwqso+RiqtnVYM01JxcXF63bp1V7yfA4kZXPfvXxl/RxsGd4i6/AOEEMJu586dNG/e3OowLJeRkUFQUBBaax555BEaN27M2LFjHd7fhY6rUmq91vrytbUF4LwxUggr7TqRxrTVh/h+w1HSc200jgxieOf6DGofRYj/5ZNIYS0ZIw1njpFlHR89HHqWSiwh2VT0jJGKnkII4ZBPPvmEtm3b0rJlS1JTU3nooYesDkkIUQk0q1WNV25pxep/9Oat21sT4OPJuLk76PzPRTz97WY2HjrDlVzQsBUUciwlm/UHzzBvy3HmbTnOmvjTxCdlkpFru6J9C1HEyjFSpnqeJz4pC4Boad4uhBAOGTt27BVd4RNCiEsJ8PHizo71uLNjPbYdTWXamkP8sPEoM9cfoXltU2jm1rZ1CC4xlTTXVsCptFyOp+ZwPDWbE6k5HE/NMd/TcjiRmk1iei6Fl8jt/L09i9tcRAT5Fv8cXuJn87tPcYP7PFsh6Tn5pOfYSM+xkZaTT3pOPmlFv2cX3ZZf4nYbubYCagSa/UUG+xJZrehnv+JtIf7eMtW1ArJyjJTE7zwJSZkE+3kRJguHhRBCCCHKtVZ1Q/jnoFie69+cHzYdZeqqQ7wwextvzN9Jx+gwTmfmcTw1m6SMvD89NsjXi9ohftQK8aNpzQhqhfgX/16rmh9gGtknpueSlGG+J9q/70/MYFV8MilZFy73H+znRX5BYakqmAb5ehHsZ76q+XkTHuSDj5cHyRl5bD6Swqm0XLLzC/70OB9PD5No2hPB4iQx2I+rIoOIaxAq7TDEOSTxO09CciYx0spBCCGEEKLCCPL1YnjnBtzVqT6bj6QybfVBth5No2Y1X1rVrUatameTuqLvwaUoLtO89qVvz7UVkJyR96cEMTkzDx8vD4J9vajm721P7LypZv9elOQF+XnheZnkTGtNZl4Bp9JyOGV/nrPfc0hMz+Xw6SzWHzzD6cyzCW6tan7c3KY2t7StS8s61eSzrZDE73zxSZm0rx9qdRhCCCGEEKKMlFK0rVedtvWqu+X5fL08qVPdnzrV/V32HEopgny9CIoIomFE0CXvm19QSFJGLmsTzjBn01Em/57AJyviaRgeyMC2dRjYps5l9yEqL0n8Ssi1FXAsJZvb2ks1TyGEEEIIUbF4e3pQO8SfgW38GdimDmcy81iw7QRzNh/lv4v38u6ivbSOCmFgmzoMaF2HWiF+Vocs3EiqepZw+HQ2hVoqegohKqZevXqxcOHCc7a9++67PPzwwxe8f8+ePSkq8d+/f39SUlL+dJ9x48Yxfvz4Sz7v7Nmz2bFjR/HvL774IosWLSpr+EIIIZwsNNCHuzrXZ8boLqx89jr+0b85WsNr83bS5c3FDJu4iulrDpGS9ec1kJWNjJFyxe8cCUmmlYNU9BRCVETDhg1jxowZ9O3bt3jbjBkzeOutty772Pnz5zv8vLNnz2bAgAG0aNECgFdeecXhfQkhhHCN2iH+jOrekFHdG3IgMYM5m48xZ9Mx/v7dVl78YRs9mkQysG0d+jSPJMDHvSlCQWEhWXkFZOUVkGczBXFU8f8V/awouUpRlbwNVfQ/PJXC28sDH0+Ft6cHnh4KpZSMkcgVv3Oc7eEniZ8QouIZPHgw8+bNIy/PnLlNSEjg2LFjTJ8+nbi4OFq2bMlLL710wcdGR0eTlJQEwOuvv06TJk245ppr2L17d/F9PvnkEzp27EibNm24/fbbycrKYuXKlcyZM4dnnnmGtm3bsn//fkaOHMnMmTMBWLx4Me3atSM2Npb777+f3Nzc4ud76aWXaN++PbGxsezatcuVh0YIIUQJDSOCeKJPExY/1YO5j17DyK7RbD2awmPTNxL32iLu/nQ1L8/dzvQ1h1h/8DSp2ReuXuoIrTU5+QWczszlyOks9pxMZ/uxNOKTMjmZlkNGro3MXBvpuUUtL2ykZttIzcrjTFYeZzLzOJ2ZR3JGHkkZeSSm55GYnsOptBxOpuVwLDWbg8mZ7D2VwY7jaew4lsaek+l06nUjc3+cx9HkNFKy8ti5ex/Hjh1j2rRpVWaMlCt+JcQnZVI9wJvqAdLKQQhxhRY8Cye2OneftWLhxjcvenNYWBidOnViwYIF3HLLLcyYMYM777yT5557jrCwMAoKCujduzdbtmyhdevWF9zH+vXrmTFjBps2bcJms9G+fXs6dOgAwG233caoUaMAeP7555k0aRJ//etfGThwIAMGDGDw4MHn7CsnJ4eRI0eyePFimjRpwr333suHH37IE088AUB4eDgbNmxgwoQJjB8/nk8//dQZR0kIIUQpKaWIjQohNiqEZ29szpr40/y45Rhbj6YyY83hc9pI1KrmR+OaQTSODKZJzSAa1wymcc0gql2mOmpyRi6bDqcQlJ3PgcQMsvMKiFw5Dv/kHfgqqKEUnh4KD4W5OoeD1UdrxaL7vUFBoSavoJB8WyF5BZr8gkLybIWoatVp2aY9M2f/SK++/Zk05Ut63XgL9zz6FGPDa+BBIfcOvpme/QbQvk0bCrWm8LzGjhV9jJTEr4SE5EyZ5imEqNCKprIUJX6TJk3im2++YeLEidhsNo4fP86OHTsumvitWLGCQYMGERBg1joPHDiw+LZt27bx/PPPk5KSQkZGxjnTZS5k9+7dxMTE0KRJEwBGjBjBBx98UDyo3XbbbQB06NCB77777or/diGEEI7z9FB0aVSDLo1qAFBYqDmaks2ek+nsOZnB3pPp7D2VwbQ1B8/pT1g7xI/GNYNpEhlEk5rBRIX6s/dUBhsPnWHj4RQOJmcB8OnA2oQVaqoH+BDs54WPjydK4XiidwFKKbw8FV6eHnCB6zij77uHuXPnMPreISydN5t3PviI336Zw5eTP8Nms3Hq5AnWbNhCjXqNyc4rYH9iBsHH07AVao6lZLHglyXcOGAgHt6+BPn5OzRGasBWWMiWbTuo3yCamlHRJGfkMnDwMCZ/8pFLx0hJ/EpISMqiU0yY1WEIISqDS1yZc6VbbrmFsWPHsmHDBrKysggLC2P8+PGsXbuW0NBQRo4cSU5OjkP7HjlyJLNnz6ZNmzZMmTKFZcuWXVGsvr6+AHh6emKz2a5oX0IIIZzLw0NRLyyAemEB9G5es3h7YaHmyBl7Qngqnb0nM9hzMp0vDySTazubEEYG+9K+fih3dapPu/qhBGWfoHHNYHPjzW+7+88Bzo6RO7duJicnm4ZRtXjovvfOGSMj/BWNI4Pw9fakRqAPQb4mXUrPKSAtx0Zqdj57TqYDcDozD9+MXI6cyeLeESP4asZMWsbG8tWXX/D78uUcPp1FRo6N46k57DqRRkGhJiUzj6NnsvE8nUlOfgEHT5vE+ExWHnkFZ4+fK8ZIWeNnl5NfwLHUbLniJ4So0IKCgujVqxf3338/w4YNIy0tjcDAQEJCQjh58iQLFiy45OO7d+/O7Nmzyc7OJj09nblz5xbflp6eTu3atcnPz2fq1KnF24ODg0lPT//Tvpo2bUpCQgL79u0D4Msvv6RHjx5O+kuFEEJYwcNDUb9GAH1a1OQvPa/inSFtmffYtex4pR+/PtOTLx/oxMpnr2P1c7356J4OPNSjEZ1iwspFA/nSjJEeHh74+3jh5aEIC/KlXlgAXh6KprWCGTzgen5fvIAIf0WgyufXRT+hgbTsfNLS0rH5hRB/Ko3p06aRYysgM9eGf2Ag2ZnpBPh4ERrgg5+PJ2GBPlwb14ZTx47gkX6S5rWr8dtP3zOgbx+X/v1yxc/O29ODhU90J9hPDokQomIbNmwYgwYNYsaMGTRr1ox27drRrFkz6tWrR7du3S752Pbt2zNkyBDatGlDZGQkHTt2LL7t1VdfpXPnzkRERNC5c+fiZG/o0KGMGjWK9957r3jBOoCfnx+TJ0/mjjvuwGaz0bFjR8aMGeOaP7oSUUr1A/4LeAKfaq3fPO92X+ALoAOQDAzRWie4O04hhCjJ00PRoEYgDcr5RZQrGSM7xcUxbOhQenTpSGRkJF06dyI8yJcWdUJ49dVXuG/QDUSER9C5cycyMjJoVrsaY+6/l1GjRjF98kRmzpyJv7cnwX7e1AqrxpQpk7nnrqFuGyOV1vry96oA4uLidFGvDSGEsMLOnTtp3ry51WFUOhc6rkqp9VrrOItCchmllCewB7geOAKsBYZprXeUuM9fgNZa6zFKqaHAIK31kEvtV8ZIIYTVZIx0vrKOjzLVUwghhCg/OgH7tNYHtNZ5wAzglvPucwvwuf3nmUBvVR7mUAkhhCjXJPETQgghyo+6wOESvx+xb7vgfbTWNiAVqHH+jpRSo5VS65RS6xITE10UrhBCiIpCEj8hhBCiEtJaT9Rax2mt4yIiIqwORwghhMUk8RNCCCeqLOumy4sqeDyPAvVK/B5l33bB+yilvIAQTJEXIYQo16rge7rLOHIsJfETQggn8fPzIzk5WQY2J9Fak5ycjJ+fn9WhuNNaoLFSKkYp5QMMBeacd585wAj7z4OBJVpedEKIck7GSOdxdHyU3gVCCOEkUVFRHDlyBFlP5Tx+fn5ERUVZHYbbaK1tSqlHgYWYdg6faa23K6VeAdZprecAk4AvlVL7gNOY5FAIIco1GSOdy5HxURI/IYRwEm9vb2JiYqwOQ1RwWuv5wPzztr1Y4ucc4A53xyWEEFdCxkjryVRPIYQQQgghhKjkJPETQgghhBBCiEpOEj8hhBBCCCGEqORUZamso5RKBA46YVfhQJIT9lPVyHFznBw7x8mxc0xlOG4NtNbSnK6UnDRGVobXjVXk2DlOjp1j5Lg5rqIfu4uOj5Um8XMWpdQ6rXWc1XFUNHLcHCfHznFy7Bwjx004Ql43jpNj5zg5do6R4+a4ynzsZKqnEEIIIYQQQlRykvgJIYQQQgghRCUnid+fTbQ6gApKjpvj5Ng5To6dY+S4CUfI68ZxcuwcJ8fOMXLcHFdpj52s8RNCCCGEEEKISk6u+AkhhBBCCCFEJSeJn51Sqp9SardSap9S6lmr46lIlFIJSqmtSqlNSql1VsdTnimlPlNKnVJKbSuxLUwp9YtSaq/9e6iVMZZHFzlu45RSR+2vu01Kqf5WxlheKaXqKaWWKqV2KKW2K6Uet2+X150oNRkjHSdjZOnI+Og4GSMdUxXHR0n8AKWUJ/ABcCPQAhimlGphbVQVTi+tddvKWv7WiaYA/c7b9iywWGvdGFhs/12cawp/Pm4A79hfd2211vPdHFNFYQOe0lq3AK4GHrG/v8nrTpSKjJFOIWPk5U1BxkdHTUHGSEdUufFREj+jE7BPa31Aa50HzABusTgmUQlprZcDp8/bfAvwuf3nz4Fb3RpUBXCR4yZKQWt9XGu9wf5zOrATqIu87kTpyRgpXE7GR8fJGOmYqjg+SuJn1AUOl/j9iH2bKB0N/KyUWq+UGm11MBVQTa31cfvPJ4CaVgZTwTyqlNpin+ZSaaZiuIpSKhpoB6xGXnei9GSMvDIyRjpO3qeujIyRpVRVxkdJ/IQzXKO1bo+ZBvSIUqq71QFVVNqU2ZVSu6XzIdAIaAscB/5tbTjlm1IqCJgFPKG1Tit5m7zuhHApGSOdQN6nykzGyFKqSuOjJH7GUaBeid+j7NtEKWitj9q/nwK+x0wLEqV3UilVG8D+/ZTF8VQIWuuTWusCrXUh8AnyursopZQ3ZlCbqrX+zr5ZXneitGSMvAIyRl4ReZ9ykIyRpVPVxkdJ/Iy1QGOlVIxSygcYCsyxOKYKQSkVqJQKLvoZuAHYdulHifPMAUbYfx4B/GBhLBVG0Zuy3SDkdXdBSikFTAJ2aq3/U+Imed2J0pIx0kEyRl4xeZ9ykIyRl1cVx0dp4G5nL3P7LuAJfKa1ft3ikCoEpVRDzBlMAC9gmhy7i1NKTQd6AuHASeAlYDbwDVAfOAjcqbWWRdolXOS49cRMYdFAAvBQiTn5wk4pdQ2wAtgKFNo3P4dZxyCvO1EqMkY6RsbI0pPx0XEyRjqmKo6PkvgJIYQQQgghRCUnUz2FEEIIIYQQopKTxE8IIYQQQgghKjlJ/IQQQgghhBCikpPETwghhBBCCCEqOUn8hBBCCCGEEKKSk8RPiHJAKVWglNpU4utZJ+47Wikl/XuEEEJUSDJGCuEcXlYHIIQAIFtr3dbqIIQQQohySMZIIZxArvgJUY4ppRKUUm8ppbYqpdYopa6yb49WSi1RSm1RSi1WStW3b6+plPpeKbXZ/tXVvitPpdQnSqntSqmflVL+lv1RQgghhBPIGClE2UjiJ0T54H/eNJYhJW5L1VrHAv8D3rVvex/4XGvdGpgKvGff/h7wq9a6DdAe+P927lBFqyCKA/j/IBsEQUSLYLBssvoEvoJBxSSmDWISX8BXsGzxNQQxCVpFsIpthd1gsCwix7AjfEFhF7/P7zL+fuWeOeEykw5n5s79OPK7SZ53940kX5Pc3vB6AGBd1EhYg+rubc8B/ntV9a27L/wm/znJre7+VFU7Sb509+WqOkpytbu/j/xBd1+pqsMk17r7eOUd15O86u7dMX6aZKe7n21+ZQDwd9RIWA8nfrB8/Yf4LI5X4h9xvxeAOaiRcEoaP+LVZKgAAACtSURBVFi+OyvPdyN+m+TuiO8neTPi10n2kqSqzlXVxX81SQDYAjUSTsmOBizD+ap6vzJ+2d2/fld9qao+5GRH8t7IPUryoqqeJDlM8mDkHyfZr6qHOdm13EtysPHZA8DmqJGwBu74wYKN+ws3u/to23MBgCVRI+FsfOoJAAAwOSd+AAAAk3PiBwAAMDmNHwAAwOQ0fgAAAJPT+AEAAExO4wcAADA5jR8AAMDkfgKF2A9yy9m3mAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 2396.18 MiB, increment: 84.57 MiB\n",
            "time: 2.51 s (started: 2023-02-14 14:41:40 +00:00)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}